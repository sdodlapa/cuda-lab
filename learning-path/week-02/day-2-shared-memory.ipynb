{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8581c1",
   "metadata": {},
   "source": [
    "## Part 1: CUDA C++ (Primary)\n",
    "\n",
    "### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94cd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e682dec",
   "metadata": {},
   "source": [
    "### What is Shared Memory?\n",
    "\n",
    "**Shared memory** is fast on-chip memory shared by all threads in a block.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                         GPU                                 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  ┌─────────────────┐  ┌─────────────────┐                   │\n",
    "│  │     Block 0     │  │     Block 1     │  ...              │\n",
    "│  │  ┌───────────┐  │  │  ┌───────────┐  │                   │\n",
    "│  │  │  Shared   │  │  │  │  Shared   │  │  ← Fast! ~5 cycles│\n",
    "│  │  │  Memory   │  │  │  │  Memory   │  │                   │\n",
    "│  │  │ (48-164KB)│  │  │  │ (48-164KB)│  │                   │\n",
    "│  │  └───────────┘  │  │  └───────────┘  │                   │\n",
    "│  │   ↑ ↑ ↑ ↑ ↑ ↑   │  │   ↑ ↑ ↑ ↑ ↑ ↑   │                   │\n",
    "│  │   T0 T1 T2...   │  │   T0 T1 T2...   │                   │\n",
    "│  └─────────────────┘  └─────────────────┘                   │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                    Global Memory                            │\n",
    "│                    (VRAM, ~400 cycles)                      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- ~100x faster than global memory\n",
    "- Shared only within a block (not across blocks)\n",
    "- Limited size: 48-164 KB per SM\n",
    "- Requires `__syncthreads()` for synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile shared_basics.cu\n",
    "/**\n",
    " * Shared Memory Basics\n",
    " * \n",
    " * Demonstrates static and dynamic shared memory allocation.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// Static shared memory: size known at compile time\n",
    "__global__ void staticShared(float *input, float *output, int n) {\n",
    "    __shared__ float cache[BLOCK_SIZE];  // Static allocation\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    // Load from global to shared\n",
    "    if (idx < n) {\n",
    "        cache[tid] = input[idx];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();  // Barrier: wait for all threads to finish loading\n",
    "    \n",
    "    // Now all threads can access any element in cache\n",
    "    // Example: each thread reads its neighbor's value\n",
    "    if (idx < n && tid > 0) {\n",
    "        output[idx] = cache[tid] + cache[tid - 1];\n",
    "    } else if (idx < n) {\n",
    "        output[idx] = cache[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Dynamic shared memory: size specified at launch\n",
    "__global__ void dynamicShared(float *input, float *output, int n) {\n",
    "    extern __shared__ float cache[];  // Size set at launch\n",
    "    \n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    if (idx < n) {\n",
    "        cache[tid] = input[idx];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    if (idx < n && tid > 0) {\n",
    "        output[idx] = cache[tid] + cache[tid - 1];\n",
    "    } else if (idx < n) {\n",
    "        output[idx] = cache[tid];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Shared Memory Basics ===\\n\\n\");\n",
    "    \n",
    "    int n = 1024;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_input = (float*)malloc(size);\n",
    "    float *h_output = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) h_input[i] = i;\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    cudaMalloc(&d_output, size);\n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    \n",
    "    // Test static shared memory\n",
    "    printf(\"Static shared memory:\\n\");\n",
    "    staticShared<<<blocks, BLOCK_SIZE>>>(d_input, d_output, n);\n",
    "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"  output[5] = %.0f + %.0f = %.0f ✓\\n\", \n",
    "           h_input[5], h_input[4], h_output[5]);\n",
    "    \n",
    "    // Test dynamic shared memory\n",
    "    printf(\"\\nDynamic shared memory:\\n\");\n",
    "    size_t sharedMemSize = BLOCK_SIZE * sizeof(float);\n",
    "    dynamicShared<<<blocks, BLOCK_SIZE, sharedMemSize>>>(d_input, d_output, n);\n",
    "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"  output[10] = %.0f + %.0f = %.0f ✓\\n\", \n",
    "           h_input[10], h_input[9], h_output[10]);\n",
    "    \n",
    "    // Query shared memory info\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"\\nDevice shared memory per block: %zu KB\\n\", \n",
    "           prop.sharedMemPerBlock / 1024);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 shared_basics.cu -o shared_basics && ./shared_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e94b4",
   "metadata": {},
   "source": [
    "### The `__syncthreads()` Barrier\n",
    "\n",
    "**Critical rule:** After loading data into shared memory, you MUST call `__syncthreads()` before any thread reads data written by another thread.\n",
    "\n",
    "```cpp\n",
    "// Thread 0 writes, Thread 1 reads\n",
    "cache[threadIdx.x] = input[idx];  // Thread 0 writes cache[0]\n",
    "__syncthreads();                  // REQUIRED!\n",
    "float val = cache[threadIdx.x - 1];  // Thread 1 reads cache[0]\n",
    "```\n",
    "\n",
    "**Warning:** `__syncthreads()` in conditional code can cause deadlocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017cc76",
   "metadata": {},
   "source": [
    "### Matrix Transpose with Shared Memory\n",
    "\n",
    "Remember from Day 1: naive transpose has uncoalesced writes. \n",
    "Solution: use shared memory as a staging buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transpose_shared.cu\n",
    "/**\n",
    " * Matrix Transpose with Shared Memory\n",
    " * \n",
    " * Uses shared memory to achieve coalesced reads AND writes.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_DIM 32\n",
    "#define BLOCK_ROWS 8\n",
    "#define WIDTH 4096\n",
    "#define HEIGHT 4096\n",
    "#define ITERATIONS 100\n",
    "\n",
    "// Naive transpose (from Day 1)\n",
    "__global__ void transposeNaive(float *out, float *in, int width, int height) {\n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    if (x < width && y < height) {\n",
    "        out[x * height + y] = in[y * width + x];  // Strided write!\n",
    "    }\n",
    "}\n",
    "\n",
    "// Shared memory transpose\n",
    "__global__ void transposeShared(float *out, float *in, int width, int height) {\n",
    "    __shared__ float tile[TILE_DIM][TILE_DIM];\n",
    "    \n",
    "    int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
    "    int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Coalesced read from global memory into shared memory\n",
    "    if (x < width && y < height) {\n",
    "        tile[threadIdx.y][threadIdx.x] = in[y * width + x];\n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Calculate new coordinates for transposed output\n",
    "    x = blockIdx.y * TILE_DIM + threadIdx.x;  // Swap block indices\n",
    "    y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
    "    \n",
    "    // Coalesced write from shared memory to global memory\n",
    "    if (x < height && y < width) {\n",
    "        out[y * height + x] = tile[threadIdx.x][threadIdx.y];  // Note: indices swapped!\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Matrix Transpose: Naive vs Shared Memory ===\\n\");\n",
    "    printf(\"Matrix: %d x %d\\n\\n\", WIDTH, HEIGHT);\n",
    "    \n",
    "    size_t size = WIDTH * HEIGHT * sizeof(float);\n",
    "    \n",
    "    float *d_in, *d_out;\n",
    "    cudaMalloc(&d_in, size);\n",
    "    cudaMalloc(&d_out, size);\n",
    "    \n",
    "    // Initialize\n",
    "    float *h_in = (float*)malloc(size);\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) h_in[i] = i;\n",
    "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_DIM, TILE_DIM);\n",
    "    dim3 grid((WIDTH + TILE_DIM - 1) / TILE_DIM, (HEIGHT + TILE_DIM - 1) / TILE_DIM);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark naive\n",
    "    transposeNaive<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        transposeNaive<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float naive_time;\n",
    "    cudaEventElapsedTime(&naive_time, start, stop);\n",
    "    naive_time /= ITERATIONS;\n",
    "    float naive_bw = 2.0f * size / (naive_time * 1e6);\n",
    "    \n",
    "    // Benchmark shared memory\n",
    "    transposeShared<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        transposeShared<<<grid, block>>>(d_out, d_in, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float shared_time;\n",
    "    cudaEventElapsedTime(&shared_time, start, stop);\n",
    "    shared_time /= ITERATIONS;\n",
    "    float shared_bw = 2.0f * size / (shared_time * 1e6);\n",
    "    \n",
    "    printf(\"❌ Naive Transpose:  %.3f ms, %.2f GB/s\\n\", naive_time, naive_bw);\n",
    "    printf(\"✅ Shared Memory:    %.3f ms, %.2f GB/s\\n\", shared_time, shared_bw);\n",
    "    printf(\"\\nSpeedup: %.2fx\\n\", naive_time / shared_time);\n",
    "    \n",
    "    // Verify correctness\n",
    "    float *h_out = (float*)malloc(size);\n",
    "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        for (int j = 0; j < 10; j++) {\n",
    "            if (h_out[j * HEIGHT + i] != h_in[i * WIDTH + j]) {\n",
    "                correct = false;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\nVerification: %s\\n\", correct ? \"✅ PASSED\" : \"❌ FAILED\");\n",
    "    \n",
    "    cudaFree(d_in);\n",
    "    cudaFree(d_out);\n",
    "    free(h_in);\n",
    "    free(h_out);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 transpose_shared.cu -o transpose_shared && ./transpose_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37347a",
   "metadata": {},
   "source": [
    "### Tiled Matrix Multiplication\n",
    "\n",
    "Shared memory really shines for matrix multiplication. Each element of C requires an entire row of A and column of B. Without shared memory, we'd reload these elements repeatedly.\n",
    "\n",
    "```\n",
    "Without tiling: Each thread loads WIDTH elements from A and B\n",
    "With tiling:    Threads cooperatively load TILE_SIZE elements, reuse them\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_tiled.cu\n",
    "/**\n",
    " * Tiled Matrix Multiplication with Shared Memory\n",
    " * \n",
    " * Demonstrates massive speedup through data reuse.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define TILE_SIZE 32\n",
    "#define M 1024\n",
    "#define N 1024\n",
    "#define K 1024\n",
    "#define ITERATIONS 10\n",
    "\n",
    "// Naive matrix multiplication (no tiling)\n",
    "__global__ void matmulNaive(float *C, float *A, float *B, int m, int n, int k) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < m && col < n) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < k; i++) {\n",
    "            sum += A[row * k + i] * B[i * n + col];\n",
    "        }\n",
    "        C[row * n + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Tiled matrix multiplication\n",
    "__global__ void matmulTiled(float *C, float *A, float *B, int m, int n, int k) {\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    // Loop over tiles\n",
    "    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; t++) {\n",
    "        // Collaboratively load tile into shared memory\n",
    "        int tiledCol = t * TILE_SIZE + threadIdx.x;\n",
    "        int tiledRow = t * TILE_SIZE + threadIdx.y;\n",
    "        \n",
    "        if (row < m && tiledCol < k)\n",
    "            As[threadIdx.y][threadIdx.x] = A[row * k + tiledCol];\n",
    "        else\n",
    "            As[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        \n",
    "        if (tiledRow < k && col < n)\n",
    "            Bs[threadIdx.y][threadIdx.x] = B[tiledRow * n + col];\n",
    "        else\n",
    "            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        // Compute partial dot product using shared memory\n",
    "        for (int i = 0; i < TILE_SIZE; i++) {\n",
    "            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < m && col < n) {\n",
    "        C[row * n + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Tiled Matrix Multiplication ===\\n\");\n",
    "    printf(\"Matrices: A(%dx%d) x B(%dx%d) = C(%dx%d)\\n\\n\", M, K, K, N, M, N);\n",
    "    \n",
    "    size_t sizeA = M * K * sizeof(float);\n",
    "    size_t sizeB = K * N * sizeof(float);\n",
    "    size_t sizeC = M * N * sizeof(float);\n",
    "    \n",
    "    float *h_A = (float*)malloc(sizeA);\n",
    "    float *h_B = (float*)malloc(sizeB);\n",
    "    float *h_C = (float*)malloc(sizeC);\n",
    "    \n",
    "    for (int i = 0; i < M * K; i++) h_A[i] = 1.0f;\n",
    "    for (int i = 0; i < K * N; i++) h_B[i] = 1.0f;\n",
    "    \n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, sizeA);\n",
    "    cudaMalloc(&d_B, sizeB);\n",
    "    cudaMalloc(&d_C, sizeC);\n",
    "    \n",
    "    cudaMemcpy(d_A, h_A, sizeA, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, sizeB, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark naive\n",
    "    matmulNaive<<<grid, block>>>(d_C, d_A, d_B, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        matmulNaive<<<grid, block>>>(d_C, d_A, d_B, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float naive_time;\n",
    "    cudaEventElapsedTime(&naive_time, start, stop);\n",
    "    naive_time /= ITERATIONS;\n",
    "    float naive_gflops = (2.0f * M * N * K) / (naive_time * 1e6);\n",
    "    \n",
    "    // Benchmark tiled\n",
    "    matmulTiled<<<grid, block>>>(d_C, d_A, d_B, M, N, K);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        matmulTiled<<<grid, block>>>(d_C, d_A, d_B, M, N, K);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float tiled_time;\n",
    "    cudaEventElapsedTime(&tiled_time, start, stop);\n",
    "    tiled_time /= ITERATIONS;\n",
    "    float tiled_gflops = (2.0f * M * N * K) / (tiled_time * 1e6);\n",
    "    \n",
    "    printf(\"❌ Naive:  %.3f ms, %.2f GFLOPS\\n\", naive_time, naive_gflops);\n",
    "    printf(\"✅ Tiled:  %.3f ms, %.2f GFLOPS\\n\", tiled_time, tiled_gflops);\n",
    "    printf(\"\\nSpeedup: %.2fx\\n\", naive_time / tiled_time);\n",
    "    \n",
    "    // Verify\n",
    "    cudaMemcpy(h_C, d_C, sizeC, cudaMemcpyDeviceToHost);\n",
    "    bool correct = true;\n",
    "    float expected = K;  // Each element should be K (sum of K 1.0s)\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        if (h_C[i] != expected) { correct = false; break; }\n",
    "    }\n",
    "    printf(\"\\nVerification: %s (expected: %.0f, got: %.0f)\\n\", \n",
    "           correct ? \"✅ PASSED\" : \"❌ FAILED\", expected, h_C[0]);\n",
    "    \n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa573909",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 matmul_tiled.cu -o matmul_tiled && ./matmul_tiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d405",
   "metadata": {},
   "source": [
    "### Shared Memory Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| `__shared__` | Declare shared memory variable |\n",
    "| `extern __shared__` | Dynamic size (set at launch) |\n",
    "| `__syncthreads()` | Barrier - all threads must reach |\n",
    "| Scope | Per-block (not visible to other blocks) |\n",
    "| Size | 48-164 KB per SM |\n",
    "\n",
    "**When to use shared memory:**\n",
    "1. Data reuse within a block (matrix multiply, convolutions)\n",
    "2. Avoiding uncoalesced access (transpose)\n",
    "3. Thread communication within a block\n",
    "4. Reducing global memory bandwidth\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Python/Numba (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ed6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "print(f\"CUDA Device: {cuda.get_current_device().name.decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared memory in Numba\n",
    "\n",
    "TILE_SIZE = 32\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_shared(C, A, B):\n",
    "    \"\"\"Tiled matrix multiplication with shared memory.\"\"\"\n",
    "    # Shared memory tiles\n",
    "    sA = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    sB = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=np.float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    row = cuda.blockIdx.y * TILE_SIZE + ty\n",
    "    col = cuda.blockIdx.x * TILE_SIZE + tx\n",
    "    \n",
    "    m, k = A.shape\n",
    "    _, n = B.shape\n",
    "    \n",
    "    result = 0.0\n",
    "    \n",
    "    for t in range((k + TILE_SIZE - 1) // TILE_SIZE):\n",
    "        # Load tile into shared memory\n",
    "        tiled_col = t * TILE_SIZE + tx\n",
    "        tiled_row = t * TILE_SIZE + ty\n",
    "        \n",
    "        if row < m and tiled_col < k:\n",
    "            sA[ty, tx] = A[row, tiled_col]\n",
    "        else:\n",
    "            sA[ty, tx] = 0.0\n",
    "            \n",
    "        if tiled_row < k and col < n:\n",
    "            sB[ty, tx] = B[tiled_row, col]\n",
    "        else:\n",
    "            sB[ty, tx] = 0.0\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Compute partial product\n",
    "        for i in range(TILE_SIZE):\n",
    "            result += sA[ty, i] * sB[i, tx]\n",
    "        \n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    if row < m and col < n:\n",
    "        C[row, col] = result\n",
    "\n",
    "# Test\n",
    "M, K, N = 512, 512, 512\n",
    "A = np.ones((M, K), dtype=np.float32)\n",
    "B = np.ones((K, N), dtype=np.float32)\n",
    "C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "threads = (TILE_SIZE, TILE_SIZE)\n",
    "blocks = ((N + TILE_SIZE - 1) // TILE_SIZE, (M + TILE_SIZE - 1) // TILE_SIZE)\n",
    "\n",
    "matmul_shared[blocks, threads](d_C, d_A, d_B)\n",
    "C = d_C.copy_to_host()\n",
    "\n",
    "print(f\"Result C[0,0] = {C[0,0]} (expected: {K})\")\n",
    "print(f\"Verification: {'✅ PASSED' if C[0,0] == K else '❌ FAILED'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
