{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7036a532",
   "metadata": {},
   "source": [
    "## Part 1: CUDA C++ (Primary)\n",
    "\n",
    "### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe26f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123d616",
   "metadata": {},
   "source": [
    "### Memory Hierarchy Review\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     GPU Memory Hierarchy                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Registers        Per-thread, fastest, ~255 per thread         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Shared Memory    Per-block, ~48-164KB, programmer-managed     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Constant Memory  64KB, cached, broadcast to warp    â† TODAY   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Texture Memory   Cached, 2D spatial locality        â† TODAY   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  L1/L2 Cache      Automatic caching                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Global Memory    Main VRAM, 4-80GB, slowest                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cbe83",
   "metadata": {},
   "source": [
    "### Constant Memory\n",
    "\n",
    "**Constant memory** is:\n",
    "- 64KB total (device-wide)\n",
    "- Cached in a special constant cache\n",
    "- **Broadcast to all threads in a warp** when they read the same address\n",
    "- Read-only from device code\n",
    "\n",
    "**Best for:** Small data that ALL threads read (like convolution kernels, lookup tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile constant_memory.cu\n",
    "/**\n",
    " * Constant Memory Demo\n",
    " * \n",
    " * Shows how to use constant memory for broadcast data.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N (1 << 22)  // 4M elements\n",
    "#define FILTER_SIZE 9\n",
    "#define ITERATIONS 100\n",
    "\n",
    "// Constant memory declaration (at file scope)\n",
    "__constant__ float d_filter[FILTER_SIZE];\n",
    "\n",
    "// Using constant memory\n",
    "__global__ void applyFilterConstant(float *input, float *output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < FILTER_SIZE; i++) {\n",
    "            // All threads read same d_filter[i] â†’ broadcast!\n",
    "            int input_idx = (idx + i) % n;\n",
    "            sum += input[input_idx] * d_filter[i];\n",
    "        }\n",
    "        output[idx] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Using global memory (for comparison)\n",
    "__global__ void applyFilterGlobal(float *input, float *output, float *filter, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < FILTER_SIZE; i++) {\n",
    "            int input_idx = (idx + i) % n;\n",
    "            sum += input[input_idx] * filter[i];  // Global memory access\n",
    "        }\n",
    "        output[idx] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Constant Memory Demo ===\\n\");\n",
    "    printf(\"Array size: %d, Filter size: %d\\n\\n\", N, FILTER_SIZE);\n",
    "    \n",
    "    // Host filter\n",
    "    float h_filter[FILTER_SIZE];\n",
    "    for (int i = 0; i < FILTER_SIZE; i++) {\n",
    "        h_filter[i] = 1.0f / FILTER_SIZE;  // Simple averaging filter\n",
    "    }\n",
    "    \n",
    "    // Copy to constant memory\n",
    "    cudaMemcpyToSymbol(d_filter, h_filter, FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    // Device memory\n",
    "    float *d_input, *d_output, *d_filter_global;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMalloc(&d_output, N * sizeof(float));\n",
    "    cudaMalloc(&d_filter_global, FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    // Initialize\n",
    "    float *h_input = (float*)malloc(N * sizeof(float));\n",
    "    for (int i = 0; i < N; i++) h_input[i] = 1.0f;\n",
    "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_filter_global, h_filter, FILTER_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark constant memory\n",
    "    applyFilterConstant<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        applyFilterConstant<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float constant_time;\n",
    "    cudaEventElapsedTime(&constant_time, start, stop);\n",
    "    constant_time /= ITERATIONS;\n",
    "    \n",
    "    // Benchmark global memory\n",
    "    applyFilterGlobal<<<blocks, threads>>>(d_input, d_output, d_filter_global, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        applyFilterGlobal<<<blocks, threads>>>(d_input, d_output, d_filter_global, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float global_time;\n",
    "    cudaEventElapsedTime(&global_time, start, stop);\n",
    "    global_time /= ITERATIONS;\n",
    "    \n",
    "    printf(\"âœ… Constant Memory: %.3f ms\\n\", constant_time);\n",
    "    printf(\"   Global Memory:   %.3f ms\\n\", global_time);\n",
    "    printf(\"\\nSpeedup: %.2fx\\n\", global_time / constant_time);\n",
    "    \n",
    "    printf(\"\\nğŸ’¡ Constant memory wins when all threads read the same data!\\n\");\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_filter_global);\n",
    "    free(h_input);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 constant_memory.cu -o constant_memory && ./constant_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96d908",
   "metadata": {},
   "source": [
    "### Constant Memory API\n",
    "\n",
    "```cpp\n",
    "// Declare at file scope (outside functions)\n",
    "__constant__ float d_data[SIZE];\n",
    "\n",
    "// Copy from host to constant memory\n",
    "cudaMemcpyToSymbol(d_data, h_data, size);\n",
    "\n",
    "// Copy from constant memory to host\n",
    "cudaMemcpyFromSymbol(h_data, d_data, size);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78be6d",
   "metadata": {},
   "source": [
    "### Texture Memory (and Modern Alternatives)\n",
    "\n",
    "**Texture memory** was traditionally used for:\n",
    "- 2D/3D spatial locality caching\n",
    "- Hardware interpolation\n",
    "- Boundary handling (clamp, wrap)\n",
    "\n",
    "**Modern approach:** Use `__ldg()` (load via texture cache) for simple read-only data.\n",
    "\n",
    "```cpp\n",
    "// Old way: complex texture object setup\n",
    "// New way: simple __ldg() intrinsic\n",
    "float val = __ldg(&input[idx]);  // Uses texture cache automatically\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82141d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ldg_cache.cu\n",
    "/**\n",
    " * Using __ldg() for read-only data\n",
    " * \n",
    " * The __ldg() intrinsic loads through the texture cache,\n",
    " * which can improve performance for read-only data.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N (1 << 24)\n",
    "#define ITERATIONS 100\n",
    "\n",
    "// Normal global memory load\n",
    "__global__ void normalLoad(const float* __restrict__ input, float *output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = input[idx] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Load through texture cache via __ldg()\n",
    "__global__ void ldgLoad(const float* __restrict__ input, float *output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        output[idx] = __ldg(&input[idx]) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== __ldg() Texture Cache Demo ===\\n\");\n",
    "    printf(\"Array size: %d elements\\n\\n\", N);\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, N * sizeof(float));\n",
    "    cudaMalloc(&d_output, N * sizeof(float));\n",
    "    \n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark normal\n",
    "    normalLoad<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        normalLoad<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float normal_time;\n",
    "    cudaEventElapsedTime(&normal_time, start, stop);\n",
    "    normal_time /= ITERATIONS;\n",
    "    \n",
    "    // Benchmark __ldg\n",
    "    ldgLoad<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < ITERATIONS; i++) {\n",
    "        ldgLoad<<<blocks, threads>>>(d_input, d_output, N);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float ldg_time;\n",
    "    cudaEventElapsedTime(&ldg_time, start, stop);\n",
    "    ldg_time /= ITERATIONS;\n",
    "    \n",
    "    printf(\"Normal load:  %.3f ms\\n\", normal_time);\n",
    "    printf(\"__ldg() load: %.3f ms\\n\", ldg_time);\n",
    "    printf(\"\\nNote: Modern compilers often auto-optimize read-only loads.\\n\");\n",
    "    printf(\"The __restrict__ keyword helps the compiler use __ldg() automatically.\\n\");\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 ldg_cache.cu -o ldg_cache && ./ldg_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3554f8c",
   "metadata": {},
   "source": [
    "### 2D Image Convolution Example\n",
    "\n",
    "Putting it all together: constant memory for filter + shared memory for tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile convolution_2d.cu\n",
    "/**\n",
    " * 2D Convolution with Constant Memory (Filter) + Shared Memory (Tile)\n",
    " * \n",
    " * A practical example combining Week 2 concepts.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define WIDTH 1024\n",
    "#define HEIGHT 1024\n",
    "#define FILTER_RADIUS 1\n",
    "#define FILTER_SIZE (2 * FILTER_RADIUS + 1)\n",
    "#define TILE_SIZE 16\n",
    "#define BLOCK_SIZE (TILE_SIZE + 2 * FILTER_RADIUS)\n",
    "\n",
    "// Filter in constant memory\n",
    "__constant__ float d_filter[FILTER_SIZE * FILTER_SIZE];\n",
    "\n",
    "// Naive convolution (no optimizations)\n",
    "__global__ void convolutionNaive(float *output, float *input, int width, int height) {\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (col < width && row < height) {\n",
    "        float sum = 0.0f;\n",
    "        for (int fy = -FILTER_RADIUS; fy <= FILTER_RADIUS; fy++) {\n",
    "            for (int fx = -FILTER_RADIUS; fx <= FILTER_RADIUS; fx++) {\n",
    "                int imgRow = min(max(row + fy, 0), height - 1);\n",
    "                int imgCol = min(max(col + fx, 0), width - 1);\n",
    "                int filterIdx = (fy + FILTER_RADIUS) * FILTER_SIZE + (fx + FILTER_RADIUS);\n",
    "                sum += input[imgRow * width + imgCol] * d_filter[filterIdx];\n",
    "            }\n",
    "        }\n",
    "        output[row * width + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized: shared memory tiling\n",
    "__global__ void convolutionTiled(float *output, float *input, int width, int height) {\n",
    "    __shared__ float tile[BLOCK_SIZE][BLOCK_SIZE];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    \n",
    "    int col = blockIdx.x * TILE_SIZE + tx - FILTER_RADIUS;\n",
    "    int row = blockIdx.y * TILE_SIZE + ty - FILTER_RADIUS;\n",
    "    \n",
    "    // Load tile with halo\n",
    "    int clampedRow = min(max(row, 0), height - 1);\n",
    "    int clampedCol = min(max(col, 0), width - 1);\n",
    "    tile[ty][tx] = input[clampedRow * width + clampedCol];\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Only interior threads compute output\n",
    "    if (tx >= FILTER_RADIUS && tx < BLOCK_SIZE - FILTER_RADIUS &&\n",
    "        ty >= FILTER_RADIUS && ty < BLOCK_SIZE - FILTER_RADIUS) {\n",
    "        \n",
    "        int outCol = blockIdx.x * TILE_SIZE + (tx - FILTER_RADIUS);\n",
    "        int outRow = blockIdx.y * TILE_SIZE + (ty - FILTER_RADIUS);\n",
    "        \n",
    "        if (outCol < width && outRow < height) {\n",
    "            float sum = 0.0f;\n",
    "            for (int fy = -FILTER_RADIUS; fy <= FILTER_RADIUS; fy++) {\n",
    "                for (int fx = -FILTER_RADIUS; fx <= FILTER_RADIUS; fx++) {\n",
    "                    int filterIdx = (fy + FILTER_RADIUS) * FILTER_SIZE + (fx + FILTER_RADIUS);\n",
    "                    sum += tile[ty + fy][tx + fx] * d_filter[filterIdx];\n",
    "                }\n",
    "            }\n",
    "            output[outRow * width + outCol] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== 2D Convolution: Naive vs Tiled ===\\n\");\n",
    "    printf(\"Image: %dx%d, Filter: %dx%d\\n\\n\", WIDTH, HEIGHT, FILTER_SIZE, FILTER_SIZE);\n",
    "    \n",
    "    // Gaussian blur filter\n",
    "    float h_filter[FILTER_SIZE * FILTER_SIZE] = {\n",
    "        1/16.0f, 2/16.0f, 1/16.0f,\n",
    "        2/16.0f, 4/16.0f, 2/16.0f,\n",
    "        1/16.0f, 2/16.0f, 1/16.0f\n",
    "    };\n",
    "    cudaMemcpyToSymbol(d_filter, h_filter, FILTER_SIZE * FILTER_SIZE * sizeof(float));\n",
    "    \n",
    "    size_t size = WIDTH * HEIGHT * sizeof(float);\n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    cudaMalloc(&d_output, size);\n",
    "    \n",
    "    // Initialize with test pattern\n",
    "    float *h_input = (float*)malloc(size);\n",
    "    for (int i = 0; i < WIDTH * HEIGHT; i++) h_input[i] = (i % 256) / 255.0f;\n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Benchmark naive\n",
    "    dim3 blockNaive(16, 16);\n",
    "    dim3 gridNaive((WIDTH + 15) / 16, (HEIGHT + 15) / 16);\n",
    "    \n",
    "    convolutionNaive<<<gridNaive, blockNaive>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        convolutionNaive<<<gridNaive, blockNaive>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float naive_time;\n",
    "    cudaEventElapsedTime(&naive_time, start, stop);\n",
    "    naive_time /= 100;\n",
    "    \n",
    "    // Benchmark tiled\n",
    "    dim3 blockTiled(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 gridTiled((WIDTH + TILE_SIZE - 1) / TILE_SIZE, (HEIGHT + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    \n",
    "    convolutionTiled<<<gridTiled, blockTiled>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaEventRecord(start);\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        convolutionTiled<<<gridTiled, blockTiled>>>(d_output, d_input, WIDTH, HEIGHT);\n",
    "    }\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    \n",
    "    float tiled_time;\n",
    "    cudaEventElapsedTime(&tiled_time, start, stop);\n",
    "    tiled_time /= 100;\n",
    "    \n",
    "    printf(\"âŒ Naive (constant mem only): %.3f ms\\n\", naive_time);\n",
    "    printf(\"âœ… Tiled (constant + shared): %.3f ms\\n\", tiled_time);\n",
    "    printf(\"\\nSpeedup: %.2fx\\n\", naive_time / tiled_time);\n",
    "    \n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53740e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -O3 convolution_2d.cu -o convolution_2d && ./convolution_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c867319",
   "metadata": {},
   "source": [
    "### Week 2 Memory Summary\n",
    "\n",
    "| Memory Type | Size | Best For | Access |\n",
    "|------------|------|----------|--------|\n",
    "| **Global** | GBs | Large data | All threads |\n",
    "| **Shared** | 48-164KB/SM | Data reuse in block | Per-block |\n",
    "| **Constant** | 64KB | Broadcast (all read same) | Read-only |\n",
    "| **Texture** | Via cache | 2D spatial locality | Read-only |\n",
    "| **Registers** | ~255/thread | Per-thread data | Per-thread |\n",
    "\n",
    "**Optimization Techniques Learned:**\n",
    "1. âœ… Memory coalescing (consecutive threads â†’ consecutive addresses)\n",
    "2. âœ… Structure of Arrays (SoA) over Array of Structures (AoS)\n",
    "3. âœ… Shared memory for data reuse\n",
    "4. âœ… `__syncthreads()` for thread synchronization\n",
    "5. âœ… Padding to avoid bank conflicts\n",
    "6. âœ… Constant memory for broadcast data\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Python/Numba (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "print(f\"CUDA Device: {cuda.get_current_device().name.decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4005b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Numba doesn't have direct constant memory support like CUDA C++\n",
    "# But you can pass small read-only arrays as kernel arguments\n",
    "\n",
    "@cuda.jit\n",
    "def apply_filter(input_arr, output_arr, filter_arr):\n",
    "    idx = cuda.grid(1)\n",
    "    n = input_arr.size\n",
    "    filter_size = filter_arr.size\n",
    "    \n",
    "    if idx < n:\n",
    "        sum_val = 0.0\n",
    "        for i in range(filter_size):\n",
    "            input_idx = (idx + i) % n\n",
    "            sum_val += input_arr[input_idx] * filter_arr[i]\n",
    "        output_arr[idx] = sum_val\n",
    "\n",
    "# Test\n",
    "N = 1 << 20\n",
    "FILTER_SIZE = 9\n",
    "\n",
    "input_arr = cuda.to_device(np.ones(N, dtype=np.float32))\n",
    "output_arr = cuda.device_array(N, dtype=np.float32)\n",
    "filter_arr = cuda.to_device(np.ones(FILTER_SIZE, dtype=np.float32) / FILTER_SIZE)\n",
    "\n",
    "threads = 256\n",
    "blocks = (N + threads - 1) // threads\n",
    "\n",
    "apply_filter[blocks, threads](input_arr, output_arr, filter_arr)\n",
    "cuda.synchronize()\n",
    "\n",
    "result = output_arr.copy_to_host()\n",
    "print(f\"Result[0] = {result[0]:.4f} (expected: 1.0)\")\n",
    "print(\"\\nNote: In Numba, small read-only arrays are automatically\")\n",
    "print(\"optimized by the compiler, similar to constant memory.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
