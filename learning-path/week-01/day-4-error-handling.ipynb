{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f6a276",
   "metadata": {},
   "source": [
    "# üöÄ Day 4: Error Handling & Debugging\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-4-error-handling.ipynb)\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU` before starting!\n",
    "\n",
    "**Structure:**\n",
    "- **Part 1: CUDA C++ (Primary)** - Native CUDA error handling\n",
    "- **Part 2: Python/Numba (Optional)** - Alternative for rapid prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9dae24",
   "metadata": {},
   "source": [
    "## Part 1: CUDA C++ (Primary)\n",
    "\n",
    "### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17458af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944057f",
   "metadata": {},
   "source": [
    "### Why Error Handling Matters\n",
    "\n",
    "CUDA operations are **asynchronous**. Errors may not appear until later!\n",
    "\n",
    "```cpp\n",
    "kernel<<<grid, block>>>(...);  // Launches, returns immediately\n",
    "// ... other code ...\n",
    "cudaDeviceSynchronize();       // Error might appear HERE!\n",
    "```\n",
    "\n",
    "### The CHECK_CUDA Macro\n",
    "\n",
    "The standard pattern for CUDA error checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile error_checking.cu\n",
    "/**\n",
    " * CUDA Error Checking Patterns\n",
    " * \n",
    " * Essential techniques for debugging CUDA code.\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// =====================================================\n",
    "// THE ESSENTIAL ERROR CHECKING MACRO\n",
    "// =====================================================\n",
    "#define CHECK_CUDA(call)                                                    \\\n",
    "    do {                                                                    \\\n",
    "        cudaError_t err = call;                                            \\\n",
    "        if (err != cudaSuccess) {                                          \\\n",
    "            fprintf(stderr, \"CUDA Error at %s:%d - %s\\n\",                  \\\n",
    "                    __FILE__, __LINE__, cudaGetErrorString(err));          \\\n",
    "            exit(EXIT_FAILURE);                                            \\\n",
    "        }                                                                  \\\n",
    "    } while (0)\n",
    "\n",
    "// Check for errors AFTER kernel launch (kernels don't return cudaError_t)\n",
    "#define CHECK_KERNEL()                                                      \\\n",
    "    do {                                                                    \\\n",
    "        cudaError_t err = cudaGetLastError();                              \\\n",
    "        if (err != cudaSuccess) {                                          \\\n",
    "            fprintf(stderr, \"Kernel Error at %s:%d - %s\\n\",                \\\n",
    "                    __FILE__, __LINE__, cudaGetErrorString(err));          \\\n",
    "            exit(EXIT_FAILURE);                                            \\\n",
    "        }                                                                  \\\n",
    "    } while (0)\n",
    "\n",
    "// A simple kernel\n",
    "__global__ void safeKernel(int *data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= 2;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== CUDA Error Checking Demo ===\\n\\n\");\n",
    "    \n",
    "    int n = 1000;\n",
    "    size_t size = n * sizeof(int);\n",
    "    \n",
    "    // Host allocation (regular C)\n",
    "    int *h_data = (int*)malloc(size);\n",
    "    for (int i = 0; i < n; i++) h_data[i] = i;\n",
    "    \n",
    "    // Device allocation WITH error checking\n",
    "    int *d_data;\n",
    "    CHECK_CUDA(cudaMalloc(&d_data, size));\n",
    "    printf(\"‚úÖ cudaMalloc successful\\n\");\n",
    "    \n",
    "    // Memory copy WITH error checking\n",
    "    CHECK_CUDA(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n",
    "    printf(\"‚úÖ cudaMemcpy (H‚ÜíD) successful\\n\");\n",
    "    \n",
    "    // Kernel launch (cannot wrap in CHECK_CUDA directly)\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    safeKernel<<<blocksPerGrid, threadsPerBlock>>>(d_data, n);\n",
    "    CHECK_KERNEL();  // Check for launch errors\n",
    "    \n",
    "    CHECK_CUDA(cudaDeviceSynchronize());  // Check for execution errors\n",
    "    printf(\"‚úÖ Kernel execution successful\\n\");\n",
    "    \n",
    "    // Copy back WITH error checking\n",
    "    CHECK_CUDA(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n",
    "    printf(\"‚úÖ cudaMemcpy (D‚ÜíH) successful\\n\");\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_data[i] != i * 2) { correct = false; break; }\n",
    "    }\n",
    "    printf(\"\\nResult: %s\\n\", correct ? \"‚úÖ CORRECT\" : \"‚ùå WRONG\");\n",
    "    \n",
    "    // Cleanup\n",
    "    CHECK_CUDA(cudaFree(d_data));\n",
    "    free(h_data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 error_checking.cu -o error_checking && ./error_checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0003a",
   "metadata": {},
   "source": [
    "### Common CUDA Errors\n",
    "\n",
    "Let's intentionally trigger some common errors to see how they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile common_errors.cu\n",
    "/**\n",
    " * Common CUDA Errors and How to Catch Them\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CHECK_CUDA(call)                                                    \\\n",
    "    do {                                                                    \\\n",
    "        cudaError_t err = call;                                            \\\n",
    "        if (err != cudaSuccess) {                                          \\\n",
    "            printf(\"‚ùå CUDA Error: %s\\n\", cudaGetErrorString(err));        \\\n",
    "            return;                                                        \\\n",
    "        }                                                                  \\\n",
    "    } while (0)\n",
    "\n",
    "__global__ void simpleKernel() {\n",
    "    // Does nothing - just for testing launch configs\n",
    "}\n",
    "\n",
    "void testInvalidConfiguration() {\n",
    "    printf(\"\\n=== Test 1: Invalid Launch Configuration ===\\n\");\n",
    "    \n",
    "    // Error: More than 1024 threads per block\n",
    "    simpleKernel<<<1, 2048>>>();  // Max is typically 1024!\n",
    "    \n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"‚ùå Caught error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "void testOutOfMemory() {\n",
    "    printf(\"\\n=== Test 2: Out of Memory ===\\n\");\n",
    "    \n",
    "    // Try to allocate 1 TB of GPU memory\n",
    "    void *ptr;\n",
    "    cudaError_t err = cudaMalloc(&ptr, 1024ULL * 1024 * 1024 * 1024);\n",
    "    \n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"‚ùå Caught error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "}\n",
    "\n",
    "void testInvalidDevice() {\n",
    "    printf(\"\\n=== Test 3: Invalid Device ===\\n\");\n",
    "    \n",
    "    // Try to use device 999\n",
    "    cudaError_t err = cudaSetDevice(999);\n",
    "    \n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"‚ùå Caught error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    \n",
    "    // Reset to device 0\n",
    "    cudaSetDevice(0);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Common CUDA Errors Demo ===\\n\");\n",
    "    \n",
    "    testInvalidConfiguration();\n",
    "    testOutOfMemory();\n",
    "    testInvalidDevice();\n",
    "    \n",
    "    printf(\"\\n‚úÖ All error tests completed!\\n\");\n",
    "    printf(\"   Key lesson: ALWAYS check CUDA return values!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff81379",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 common_errors.cu -o common_errors && ./common_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c75b42",
   "metadata": {},
   "source": [
    "### Using compute-sanitizer (Memory Checker)\n",
    "\n",
    "The `compute-sanitizer` tool (replaces cuda-memcheck) detects memory errors at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d02cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile memory_bug.cu\n",
    "/**\n",
    " * A program with an intentional out-of-bounds memory access\n",
    " * Use compute-sanitizer to detect it!\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void buggyKernel(int *data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // BUG: No bounds check! Will access invalid memory!\n",
    "    data[idx] = idx;  // idx can exceed n!\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 100;\n",
    "    int *d_data;\n",
    "    cudaMalloc(&d_data, n * sizeof(int));\n",
    "    \n",
    "    // Launch with MORE threads than array elements\n",
    "    // This will cause out-of-bounds writes\n",
    "    buggyKernel<<<1, 256>>>(d_data, n);  // 256 threads, but only 100 elements!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaFree(d_data);\n",
    "    printf(\"Program completed (but had memory errors!)\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02897621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with debug info for better error messages\n",
    "!nvcc -arch=sm_75 -g -G memory_bug.cu -o memory_bug\n",
    "\n",
    "# Run with compute-sanitizer (if available)\n",
    "!which compute-sanitizer && compute-sanitizer ./memory_bug || echo \"compute-sanitizer not available in Colab, but would detect: Invalid __global__ write\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c7dd5",
   "metadata": {},
   "source": [
    "### CUDA C++ Error Handling Summary\n",
    "\n",
    "| Pattern | When to Use |\n",
    "|---------|-------------|\n",
    "| `CHECK_CUDA(call)` | Wrap all CUDA API calls |\n",
    "| `CHECK_KERNEL()` | After kernel launches |\n",
    "| `cudaGetLastError()` | Get most recent error |\n",
    "| `cudaGetErrorString(err)` | Convert error to text |\n",
    "| `cudaDeviceSynchronize()` | Force sync and catch async errors |\n",
    "| `compute-sanitizer` | Detect memory errors at runtime |\n",
    "\n",
    "**Essential Error Checking Rules:**\n",
    "1. **Always** check return values of CUDA API calls\n",
    "2. **Always** call `cudaGetLastError()` after kernel launches\n",
    "3. **Always** sync before checking results\n",
    "4. Use debug builds (`-g -G`) during development\n",
    "5. Test with `compute-sanitizer` regularly\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Python/Numba (Optional Comparison)\n",
    "\n",
    "The following sections demonstrate error handling using Python and Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Colab Setup Cell - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üîß Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"üíª Running locally - make sure you have: pip install numba numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b16164",
   "metadata": {},
   "source": [
    "# Day 4: Error Handling & Debugging\n",
    "\n",
    "Bugs in CUDA code can be subtle and hard to find. Today you'll learn:\n",
    "- How CUDA errors work\n",
    "- Proper error checking patterns\n",
    "- Common pitfalls and how to avoid them\n",
    "- Debugging techniques\n",
    "- Using CUDA-MEMCHECK and compute-sanitizer\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Understanding CUDA Errors\n",
    "\n",
    "CUDA operations can fail for many reasons:\n",
    "- Invalid kernel launch configuration\n",
    "- Out of memory\n",
    "- Device not available\n",
    "- Invalid memory access\n",
    "- Race conditions\n",
    "\n",
    "**Key concept:** CUDA operations are often **asynchronous**. Errors may not be reported until later!\n",
    "\n",
    "```\n",
    "kernel<<<grid, block>>>(...);  // Launches, returns immediately\n",
    "// ... other code ...\n",
    "cudaDeviceSynchronize();       // Error might appear HERE!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba.cuda.cudadrv.driver import CudaAPIError\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "print(\"CUDA device:\", cuda.get_current_device().name.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51604cf3",
   "metadata": {},
   "source": [
    "## 2. Common CUDA Errors & How to Trigger Them\n",
    "\n",
    "Let's intentionally cause errors to understand how they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf918e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 1: Invalid Launch Configuration\n",
    "# Max threads per block is 1024, what happens if we exceed it?\n",
    "\n",
    "@cuda.jit\n",
    "def simple_kernel(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        arr[idx] = idx\n",
    "\n",
    "arr = np.zeros(100, dtype=np.float32)\n",
    "arr_d = cuda.to_device(arr)\n",
    "\n",
    "print(\"Attempting to launch with 2048 threads per block...\")\n",
    "print(\"(Max allowed is 1024)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # This will fail - too many threads per block!\n",
    "    simple_kernel[1, 2048](arr_d)\n",
    "    cuda.synchronize()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 2: Out of Memory\n",
    "# Trying to allocate more than available GPU memory\n",
    "\n",
    "ctx = cuda.current_context()\n",
    "free_mem, total_mem = ctx.get_memory_info()\n",
    "print(f\"Free GPU memory: {free_mem / 1e9:.2f} GB\")\n",
    "print(f\"Attempting to allocate: {free_mem * 2 / 1e9:.2f} GB (2x available)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Try to allocate more than available\n",
    "    huge_array = cuda.device_array(int(free_mem * 2), dtype=np.uint8)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error caught: {type(e).__name__}\")\n",
    "    print(f\"   Message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df5a20",
   "metadata": {},
   "source": [
    "## 3. The Debugging Checklist\n",
    "\n",
    "When your CUDA code doesn't work, check these in order:\n",
    "\n",
    "### üîç Checklist\n",
    "\n",
    "1. **Is CUDA available?**\n",
    "   ```python\n",
    "   cuda.is_available()\n",
    "   ```\n",
    "\n",
    "2. **Are launch parameters valid?**\n",
    "   - `threads_per_block` ‚â§ 1024\n",
    "   - `blocks` > 0\n",
    "   - Grid dimensions within limits\n",
    "\n",
    "3. **Is there enough memory?**\n",
    "   - Check `cuda.current_context().get_memory_info()`\n",
    "\n",
    "4. **Are array sizes correct?**\n",
    "   - Boundary checks in kernel: `if idx < n:`\n",
    "\n",
    "5. **Are data types matching?**\n",
    "   - GPU prefers float32, not float64\n",
    "\n",
    "6. **Did you synchronize?**\n",
    "   - `cuda.synchronize()` before reading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Safe kernel launch wrapper\n",
    "def safe_launch(kernel, grid, block, *args, **kwargs):\n",
    "    \"\"\"Launch kernel with error checking\"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    \n",
    "    # Validate block size\n",
    "    if isinstance(block, int):\n",
    "        block = (block,)\n",
    "    total_threads = 1\n",
    "    for dim in block:\n",
    "        total_threads *= dim\n",
    "    if total_threads > device.MAX_THREADS_PER_BLOCK:\n",
    "        raise ValueError(f\"Block size {block} = {total_threads} threads exceeds max {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    \n",
    "    # Validate grid size\n",
    "    if isinstance(grid, int):\n",
    "        grid = (grid,)\n",
    "    for i, dim in enumerate(grid):\n",
    "        max_dim = [device.MAX_GRID_DIM_X, device.MAX_GRID_DIM_Y, device.MAX_GRID_DIM_Z][i]\n",
    "        if dim > max_dim:\n",
    "            raise ValueError(f\"Grid dimension {i} = {dim} exceeds max {max_dim}\")\n",
    "    \n",
    "    # Launch\n",
    "    kernel[grid, block](*args, **kwargs)\n",
    "    cuda.synchronize()\n",
    "\n",
    "# Test safe launch\n",
    "print(\"Testing safe_launch helper:\")\n",
    "arr = cuda.device_array(100, dtype=np.float32)\n",
    "\n",
    "try:\n",
    "    safe_launch(simple_kernel, 1, 2048, arr)  # Should fail validation\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Caught before launch: {e}\")\n",
    "\n",
    "safe_launch(simple_kernel, 1, 256, arr)  # Should work\n",
    "print(\"‚úÖ Valid launch succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441f135",
   "metadata": {},
   "source": [
    "## 4. Common Pitfalls & Bug Patterns\n",
    "\n",
    "### Pitfall 1: Missing Boundary Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ab03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: No boundary check\n",
    "@cuda.jit\n",
    "def bad_kernel_no_bounds(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    arr[idx] = idx  # üí• Will access out-of-bounds memory!\n",
    "\n",
    "# GOOD: With boundary check\n",
    "@cuda.jit  \n",
    "def good_kernel_with_bounds(arr, n):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n:  # ‚úÖ Always check!\n",
    "        arr[idx] = idx\n",
    "\n",
    "# Demonstrate the difference\n",
    "n = 100\n",
    "arr = cuda.device_array(n, dtype=np.float32)\n",
    "threads = 256  # More threads than elements!\n",
    "blocks = 1\n",
    "\n",
    "print(\"With proper bounds checking:\")\n",
    "good_kernel_with_bounds[blocks, threads](arr, n)\n",
    "cuda.synchronize()\n",
    "print(\"‚úÖ Completed safely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0461cc81",
   "metadata": {},
   "source": [
    "### Pitfall 2: Wrong Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy defaults to float64, but CUDA prefers float32\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def add_arrays(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "n = 10_000_000\n",
    "\n",
    "# float64 (default) - slower on most GPUs\n",
    "a64 = np.random.randn(n)  # Default is float64!\n",
    "b64 = np.random.randn(n)\n",
    "c64 = np.zeros(n)\n",
    "\n",
    "# float32 - preferred\n",
    "a32 = np.random.randn(n).astype(np.float32)\n",
    "b32 = np.random.randn(n).astype(np.float32)\n",
    "c32 = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "threads, blocks = 256, math.ceil(n / 256)\n",
    "\n",
    "# Benchmark float64\n",
    "a64_d, b64_d = cuda.to_device(a64), cuda.to_device(b64)\n",
    "c64_d = cuda.device_array(n, dtype=np.float64)\n",
    "add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
    "cuda.synchronize()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    add_arrays[blocks, threads](a64_d, b64_d, c64_d)\n",
    "cuda.synchronize()\n",
    "time64 = (time.perf_counter() - start) / 10\n",
    "\n",
    "# Benchmark float32\n",
    "a32_d, b32_d = cuda.to_device(a32), cuda.to_device(b32)\n",
    "c32_d = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    add_arrays[blocks, threads](a32_d, b32_d, c32_d)\n",
    "cuda.synchronize()\n",
    "time32 = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"float64: {time64*1000:.3f} ms\")\n",
    "print(f\"float32: {time32*1000:.3f} ms\")\n",
    "print(f\"Speedup: {time64/time32:.2f}x\")\n",
    "print(\"\\nüí° Tip: Always use .astype(np.float32) unless you need float64 precision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f0cfe",
   "metadata": {},
   "source": [
    "### Pitfall 3: Forgetting to Synchronize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel execution is ASYNCHRONOUS\n",
    "@cuda.jit\n",
    "def slow_kernel(arr):\n",
    "    \"\"\"Simulate slow computation\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        # Busy work\n",
    "        val = 0.0\n",
    "        for i in range(1000):\n",
    "            val += idx * 0.001\n",
    "        arr[idx] = val\n",
    "\n",
    "arr = cuda.device_array(10000, dtype=np.float32)\n",
    "threads, blocks = 256, math.ceil(10000 / 256)\n",
    "\n",
    "# BAD: Timing without synchronization\n",
    "start = time.perf_counter()\n",
    "slow_kernel[blocks, threads](arr)\n",
    "# Missing: cuda.synchronize()\n",
    "bad_time = time.perf_counter() - start\n",
    "print(f\"Without sync: {bad_time*1000:.3f} ms (WRONG! Kernel still running)\")\n",
    "\n",
    "# GOOD: Proper timing with synchronization  \n",
    "start = time.perf_counter()\n",
    "slow_kernel[blocks, threads](arr)\n",
    "cuda.synchronize()  # Wait for kernel to complete\n",
    "good_time = time.perf_counter() - start\n",
    "print(f\"With sync:    {good_time*1000:.3f} ms (Correct)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è The unsynchronized time is {good_time/bad_time:.0f}x too fast!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d257e0",
   "metadata": {},
   "source": [
    "## 5. Debugging with Print Statements\n",
    "\n",
    "In Numba CUDA, you can use `print()` inside kernels for debugging (but use sparingly - it's slow!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def debug_kernel(arr, n):\n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # Only print from first few threads to avoid output flood\n",
    "    if idx < 3:\n",
    "        print(\"Thread\", idx, \"starting\")\n",
    "    \n",
    "    if idx < n:\n",
    "        arr[idx] = idx * 2\n",
    "        \n",
    "        # Debug: Print values for first few elements\n",
    "        if idx < 3:\n",
    "            print(\"Thread\", idx, \"wrote value\", arr[idx])\n",
    "\n",
    "# Run with small array\n",
    "arr = cuda.device_array(10, dtype=np.float32)\n",
    "debug_kernel[1, 10](arr, 10)\n",
    "cuda.synchronize()\n",
    "\n",
    "print(\"\\nFinal array:\", arr.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3183d",
   "metadata": {},
   "source": [
    "## üéØ Exercises\n",
    "\n",
    "### Exercise 1: Error-Proof Kernel Wrapper\n",
    "Create a robust wrapper function that validates all inputs before launching a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Complete this error-proof wrapper\n",
    "\n",
    "def launch_kernel_safe(kernel, data, threads_per_block=256):\n",
    "    \"\"\"\n",
    "    Safely launch a kernel with automatic configuration and error checking.\n",
    "    \n",
    "    Args:\n",
    "        kernel: The CUDA kernel function\n",
    "        data: Input array (numpy or device array)\n",
    "        threads_per_block: Threads per block (default 256)\n",
    "    \n",
    "    Returns:\n",
    "        Device array with results\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If inputs are invalid\n",
    "        MemoryError: If not enough GPU memory\n",
    "    \"\"\"\n",
    "    # TODO: Implement the following checks:\n",
    "    # 1. Verify CUDA is available\n",
    "    # 2. Check data is not empty\n",
    "    # 3. Validate threads_per_block (1-1024)\n",
    "    # 4. Check sufficient GPU memory\n",
    "    # 5. Launch kernel with proper grid configuration\n",
    "    # 6. Synchronize and check for errors\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762d8c3",
   "metadata": {},
   "source": [
    "## üìù Key Takeaways\n",
    "\n",
    "### Error Handling Best Practices:\n",
    "\n",
    "1. **Always synchronize** before reading results or timing\n",
    "   ```python\n",
    "   kernel[grid, block](...)\n",
    "   cuda.synchronize()  # Wait for completion\n",
    "   result = output.copy_to_host()\n",
    "   ```\n",
    "\n",
    "2. **Validate launch configuration**\n",
    "   - threads_per_block ‚â§ 1024\n",
    "   - Check grid dimensions against device limits\n",
    "\n",
    "3. **Always include boundary checks**\n",
    "   ```python\n",
    "   if idx < n:\n",
    "       arr[idx] = ...\n",
    "   ```\n",
    "\n",
    "4. **Use try/except for error handling**\n",
    "   ```python\n",
    "   try:\n",
    "       kernel[grid, block](...)\n",
    "   except CudaAPIError as e:\n",
    "       print(f\"CUDA Error: {e}\")\n",
    "   ```\n",
    "\n",
    "5. **Prefer float32** unless you need float64 precision\n",
    "\n",
    "6. **Debug strategically**\n",
    "   - Use print() sparingly (only first few threads)\n",
    "   - Use small test cases first\n",
    "   - Verify CPU results before GPU\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "You've completed Week 1! Before moving on:\n",
    "1. Complete the checkpoint quiz\n",
    "2. Finish all exercises in each notebook\n",
    "3. Make sure you can run all code without errors\n",
    "\n",
    "### üîó Resources\n",
    "- [Error Handling Guide](../../cuda-programming-guide/02-basics/nvcc.md)\n",
    "- [Debugging Documentation](../../cuda-programming-guide/04-special-topics/error-log-management.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
