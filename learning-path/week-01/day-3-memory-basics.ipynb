{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea28ffed",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 3: GPU Memory Fundamentals\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sdodlapa/cuda-lab/blob/main/learning-path/week-01/day-3-memory-basics.ipynb)\n",
    "\n",
    "> **Note:** If running on Google Colab, go to `Runtime â†’ Change runtime type â†’ T4 GPU` before starting!\n",
    "\n",
    "**Structure:**\n",
    "- **Part 1: CUDA C++ (Primary)** - Native CUDA memory management\n",
    "- **Part 2: Python/Numba (Optional)** - Alternative for rapid prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c5c55",
   "metadata": {},
   "source": [
    "## Part 1: CUDA C++ (Primary)\n",
    "\n",
    "### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b04af",
   "metadata": {},
   "source": [
    "### The GPU Memory Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        GPU MEMORY HIERARCHY                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Registers (per thread)     â† Fastest (~0 cycles)              â”‚\n",
    "â”‚   Shared Memory (per block)  â† Very fast (~5 cycles)            â”‚\n",
    "â”‚   L1/L2 Cache               â† Fast (~30-200 cycles)            â”‚\n",
    "â”‚   Global Memory (VRAM)       â† Slow (~400+ cycles)              â”‚\n",
    "â”‚   Host Memory (System RAM)   â† Very slow (PCIe transfer)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Explicit Memory Management\n",
    "\n",
    "The traditional CUDA pattern: allocate, copy, compute, copy back, free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile explicit_memory.cu\n",
    "/**\n",
    " * Explicit Memory Management in CUDA\n",
    " * \n",
    " * The classic pattern: cudaMalloc â†’ cudaMemcpy â†’ kernel â†’ cudaMemcpy â†’ cudaFree\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void vectorScale(float *data, float scale, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;  // 1 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    printf(\"=== Explicit Memory Management ===\\n\");\n",
    "    printf(\"Array size: %d elements (%.2f MB)\\n\\n\", n, size / (1024.0 * 1024.0));\n",
    "    \n",
    "    // Step 1: Allocate HOST memory\n",
    "    float *h_data = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize data\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_data[i] = 1.0f;\n",
    "    }\n",
    "    \n",
    "    // Step 2: Allocate DEVICE memory\n",
    "    float *d_data;\n",
    "    cudaMalloc(&d_data, size);\n",
    "    printf(\"Step 1: Allocated %.2f MB on GPU\\n\", size / (1024.0 * 1024.0));\n",
    "    \n",
    "    // Step 3: Copy data Host â†’ Device\n",
    "    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);\n",
    "    printf(\"Step 2: Copied data to GPU\\n\");\n",
    "    \n",
    "    // Step 4: Execute kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    vectorScale<<<blocksPerGrid, threadsPerBlock>>>(d_data, 2.0f, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Step 3: Executed kernel (scale by 2.0)\\n\");\n",
    "    \n",
    "    // Step 5: Copy result Device â†’ Host\n",
    "    cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost);\n",
    "    printf(\"Step 4: Copied result back to CPU\\n\");\n",
    "    \n",
    "    // Step 6: Free device memory\n",
    "    cudaFree(d_data);\n",
    "    printf(\"Step 5: Freed GPU memory\\n\");\n",
    "    \n",
    "    // Verify result\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_data[i] != 2.0f) { correct = false; break; }\n",
    "    }\n",
    "    printf(\"\\nResult: %s\\n\", correct ? \"âœ… CORRECT\" : \"âŒ WRONG\");\n",
    "    \n",
    "    free(h_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd50cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc explicit_memory.cu -o explicit_memory && ./explicit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1051df",
   "metadata": {},
   "source": [
    "### Unified Memory (cudaMallocManaged)\n",
    "\n",
    "Unified Memory simplifies programming by creating a single address space accessible from both CPU and GPU. The driver automatically migrates data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd66a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile unified_memory.cu\n",
    "/**\n",
    " * Unified Memory - Simplified Memory Management\n",
    " * \n",
    " * cudaMallocManaged creates memory accessible from both CPU and GPU.\n",
    " * No explicit cudaMemcpy needed!\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void vectorScale(float *data, float scale, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        data[idx] *= scale;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    printf(\"=== Unified Memory (Managed) ===\\n\");\n",
    "    printf(\"Array size: %d elements (%.2f MB)\\n\\n\", n, size / (1024.0 * 1024.0));\n",
    "    \n",
    "    // Allocate UNIFIED memory (accessible from both CPU and GPU!)\n",
    "    float *data;\n",
    "    cudaMallocManaged(&data, size);\n",
    "    printf(\"Step 1: Allocated unified memory\\n\");\n",
    "    \n",
    "    // Initialize on CPU (no copy needed)\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        data[i] = 1.0f;\n",
    "    }\n",
    "    printf(\"Step 2: Initialized data on CPU\\n\");\n",
    "    \n",
    "    // Execute kernel (data automatically migrates to GPU)\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    vectorScale<<<blocksPerGrid, threadsPerBlock>>>(data, 3.0f, n);\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Step 3: Executed kernel (scale by 3.0)\\n\");\n",
    "    \n",
    "    // Access result on CPU (data automatically migrates back)\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (data[i] != 3.0f) { correct = false; break; }\n",
    "    }\n",
    "    printf(\"Step 4: Verified result on CPU\\n\");\n",
    "    printf(\"\\nResult: %s\\n\", correct ? \"âœ… CORRECT\" : \"âŒ WRONG\");\n",
    "    \n",
    "    // Free unified memory\n",
    "    cudaFree(data);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbf377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc unified_memory.cu -o unified_memory && ./unified_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5424a7",
   "metadata": {},
   "source": [
    "### Pinned (Page-Locked) Memory\n",
    "\n",
    "Pinned memory enables faster DMA transfers and async operations. Regular `malloc` memory can be swapped to disk; pinned memory cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pinned_memory.cu\n",
    "/**\n",
    " * Pinned (Page-Locked) Memory\n",
    " * \n",
    " * Pinned memory enables:\n",
    " * - Faster DMA transfers\n",
    " * - Asynchronous memory copies (with streams)\n",
    " * - Memory mapped for direct GPU access\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    printf(\"=== Pinned vs Pageable Memory Comparison ===\\n\");\n",
    "    printf(\"Array size: %d elements (%.2f MB)\\n\\n\", n, size / (1024.0 * 1024.0));\n",
    "    \n",
    "    // ========== Test 1: Pageable Memory (Regular malloc) ==========\n",
    "    float *h_a_pageable = (float*)malloc(size);\n",
    "    float *h_b_pageable = (float*)malloc(size);\n",
    "    float *h_c_pageable = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a_pageable[i] = 1.0f;\n",
    "        h_b_pageable[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    cudaMemcpy(d_a, h_a_pageable, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b_pageable, size, cudaMemcpyHostToDevice);\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    float pageable_time = std::chrono::duration<float, std::milli>(end - start).count();\n",
    "    printf(\"Pageable memory transfer time: %.2f ms\\n\", pageable_time);\n",
    "    \n",
    "    // ========== Test 2: Pinned Memory (cudaMallocHost) ==========\n",
    "    float *h_a_pinned, *h_b_pinned, *h_c_pinned;\n",
    "    cudaMallocHost(&h_a_pinned, size);  // Pinned allocation\n",
    "    cudaMallocHost(&h_b_pinned, size);\n",
    "    cudaMallocHost(&h_c_pinned, size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a_pinned[i] = 1.0f;\n",
    "        h_b_pinned[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    start = std::chrono::high_resolution_clock::now();\n",
    "    cudaMemcpy(d_a, h_a_pinned, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b_pinned, size, cudaMemcpyHostToDevice);\n",
    "    end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    float pinned_time = std::chrono::duration<float, std::milli>(end - start).count();\n",
    "    printf(\"Pinned memory transfer time:   %.2f ms\\n\", pinned_time);\n",
    "    printf(\"Speedup: %.2fx\\n\", pageable_time / pinned_time);\n",
    "    \n",
    "    // Cleanup\n",
    "    free(h_a_pageable); free(h_b_pageable); free(h_c_pageable);\n",
    "    cudaFreeHost(h_a_pinned); cudaFreeHost(h_b_pinned); cudaFreeHost(h_c_pinned);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc pinned_memory.cu -o pinned_memory && ./pinned_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ac260",
   "metadata": {},
   "source": [
    "### CUDA C++ Memory API Summary\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `cudaMalloc(&ptr, size)` | Allocate device (GPU) memory |\n",
    "| `cudaFree(ptr)` | Free device memory |\n",
    "| `cudaMemcpy(dst, src, size, kind)` | Copy between host/device |\n",
    "| `cudaMallocManaged(&ptr, size)` | Allocate unified memory |\n",
    "| `cudaMallocHost(&ptr, size)` | Allocate pinned host memory |\n",
    "| `cudaFreeHost(ptr)` | Free pinned host memory |\n",
    "\n",
    "**Memory Copy Kinds:**\n",
    "- `cudaMemcpyHostToDevice` - CPU â†’ GPU\n",
    "- `cudaMemcpyDeviceToHost` - GPU â†’ CPU\n",
    "- `cudaMemcpyDeviceToDevice` - GPU â†’ GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Python/Numba (Optional Comparison)\n",
    "\n",
    "The following sections demonstrate memory management using Python and Numba. This is useful for prototyping, but production code typically uses CUDA C++ for performance-critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164740f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Colab Setup Cell - Run this first!\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸ”§ Running on Google Colab - Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numba\"])\n",
    "    print(\"âœ… Setup complete!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running locally - make sure you have: pip install numba numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5957e72",
   "metadata": {},
   "source": [
    "# Day 3: GPU Memory Fundamentals\n",
    "\n",
    "Memory management is **the most important skill** in CUDA programming. Today you'll learn:\n",
    "- GPU memory hierarchy\n",
    "- Explicit memory management (cudaMalloc, cudaMemcpy)\n",
    "- Unified/Managed memory (cudaMallocManaged)\n",
    "- Pinned vs pageable memory for transfers\n",
    "- Performance implications of each approach\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Memory Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        GPU MEMORY HIERARCHY                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Registers (per thread)     â† Fastest (~0 latency)             â”‚\n",
    "â”‚   â”œâ”€â”€ ~255 32-bit registers per thread                          â”‚\n",
    "â”‚   â””â”€â”€ Private to each thread                                    â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Shared Memory (per block)  â† Very fast (~5 cycles)            â”‚\n",
    "â”‚   â”œâ”€â”€ 48-164 KB per SM                                          â”‚\n",
    "â”‚   â””â”€â”€ Shared between threads in same block                      â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L1 Cache / Texture Cache   â† Fast (~30 cycles)                â”‚\n",
    "â”‚   â””â”€â”€ Automatic caching                                         â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   L2 Cache                   â† Medium (~200 cycles)             â”‚\n",
    "â”‚   â””â”€â”€ Shared across all SMs                                     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Global Memory (VRAM)       â† Slow (~400+ cycles)              â”‚\n",
    "â”‚   â”œâ”€â”€ 4-80 GB (main GPU memory)                                 â”‚\n",
    "â”‚   â””â”€â”€ Accessible by all threads                                 â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Host Memory (System RAM)   â† Very slow (PCIe transfer)        â”‚\n",
    "â”‚   â””â”€â”€ Must be copied to GPU before use                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(\"CUDA device:\", cuda.get_current_device().name.decode())\n",
    "\n",
    "# Get memory info\n",
    "ctx = cuda.current_context()\n",
    "free_mem, total_mem = ctx.get_memory_info()\n",
    "print(f\"GPU Memory: {free_mem/1e9:.2f} GB free / {total_mem/1e9:.2f} GB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3276b7",
   "metadata": {},
   "source": [
    "## 2. Explicit Memory Management\n",
    "\n",
    "The traditional CUDA workflow requires **explicit** memory transfers:\n",
    "\n",
    "```\n",
    "1. Allocate memory on GPU (cudaMalloc)\n",
    "2. Copy data from CPU to GPU (cudaMemcpy H2D)\n",
    "3. Launch kernel\n",
    "4. Copy results from GPU to CPU (cudaMemcpy D2H)\n",
    "5. Free GPU memory (cudaFree)\n",
    "```\n",
    "\n",
    "In Numba:\n",
    "- `cuda.to_device(array)` - Allocates and copies to GPU\n",
    "- `cuda.device_array(shape, dtype)` - Allocates on GPU only\n",
    "- `device_array.copy_to_host()` - Copies back to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def saxpy_kernel(a, x, y, out, n):\n",
    "    \"\"\"SAXPY: out = a*x + y (Single-precision A*X Plus Y)\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n:\n",
    "        out[idx] = a * x[idx] + y[idx]\n",
    "\n",
    "def explicit_memory_workflow(n, a=2.0):\n",
    "    \"\"\"Demonstrate explicit memory management\"\"\"\n",
    "    print(f\"\\nğŸ“Š Explicit Memory Workflow (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 1: Create data on CPU\n",
    "    x_host = np.random.randn(n).astype(np.float32)\n",
    "    y_host = np.random.randn(n).astype(np.float32)\n",
    "    out_host = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    # Step 2: Allocate and copy to GPU\n",
    "    start = time.perf_counter()\n",
    "    x_device = cuda.to_device(x_host)        # Allocate + copy\n",
    "    y_device = cuda.to_device(y_host)        # Allocate + copy\n",
    "    out_device = cuda.device_array(n, dtype=np.float32)  # Allocate only\n",
    "    h2d_time = time.perf_counter() - start\n",
    "    print(f\"  Hostâ†’Device transfer: {h2d_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Step 3: Launch kernel\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    saxpy_kernel[blocks, threads](a, x_device, y_device, out_device, n)\n",
    "    cuda.synchronize()\n",
    "    kernel_time = time.perf_counter() - start\n",
    "    print(f\"  Kernel execution:      {kernel_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Step 4: Copy result back\n",
    "    start = time.perf_counter()\n",
    "    out_host = out_device.copy_to_host()\n",
    "    d2h_time = time.perf_counter() - start\n",
    "    print(f\"  Deviceâ†’Host transfer:  {d2h_time*1000:.3f} ms\")\n",
    "    \n",
    "    print(f\"  Total time:            {(h2d_time + kernel_time + d2h_time)*1000:.3f} ms\")\n",
    "    \n",
    "    # Verify\n",
    "    expected = a * x_host + y_host\n",
    "    print(f\"  âœ… Correct: {np.allclose(out_host, expected)}\")\n",
    "    \n",
    "    return h2d_time, kernel_time, d2h_time\n",
    "\n",
    "# Run with different sizes\n",
    "for size in [100_000, 1_000_000, 10_000_000]:\n",
    "    explicit_memory_workflow(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e73a4",
   "metadata": {},
   "source": [
    "## 3. Unified Memory (cudaMallocManaged)\n",
    "\n",
    "**Unified Memory** simplifies programming - memory is automatically migrated between CPU and GPU as needed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     CPU      â”‚â—„â”€â”€â”€â”€ Automatic â”€â”€â”€â”€â–ºâ”‚     GPU      â”‚\n",
    "â”‚              â”‚      Migration      â”‚              â”‚\n",
    "â”‚  ptr[i] = x  â”‚                    â”‚  ptr[i] = y  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        Same pointer works on both!\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simpler code\n",
    "- No explicit transfers needed\n",
    "- Can access more data than GPU memory (oversubscription)\n",
    "\n",
    "**Cons:**\n",
    "- May have higher latency (page faults)\n",
    "- Less control over when transfers happen\n",
    "- Can be slower if not used carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_memory_workflow(n, a=2.0):\n",
    "    \"\"\"Demonstrate unified memory (managed memory)\"\"\"\n",
    "    print(f\"\\nğŸ“Š Unified Memory Workflow (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Allocate managed memory - accessible from both CPU and GPU\n",
    "    x = cuda.managed_array(n, dtype=np.float32)\n",
    "    y = cuda.managed_array(n, dtype=np.float32)\n",
    "    out = cuda.managed_array(n, dtype=np.float32)\n",
    "    \n",
    "    # Initialize on CPU (memory will migrate to GPU when needed)\n",
    "    x[:] = np.random.randn(n).astype(np.float32)\n",
    "    y[:] = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Launch kernel (data migrates automatically)\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    saxpy_kernel[blocks, threads](a, x, y, out, n)\n",
    "    cuda.synchronize()\n",
    "    total_time = time.perf_counter() - start\n",
    "    print(f\"  Kernel + migration time: {total_time*1000:.3f} ms\")\n",
    "    \n",
    "    # Access result on CPU (data migrates back automatically)\n",
    "    expected = a * x + y\n",
    "    print(f\"  âœ… Correct: {np.allclose(out, expected)}\")\n",
    "    \n",
    "    return total_time\n",
    "\n",
    "# Compare with explicit management\n",
    "for size in [100_000, 1_000_000, 10_000_000]:\n",
    "    unified_memory_workflow(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f7062",
   "metadata": {},
   "source": [
    "## 4. Pinned (Page-Locked) Memory\n",
    "\n",
    "Normal CPU memory can be **paged out** to disk by the OS. This causes problems for DMA transfers.\n",
    "\n",
    "**Pinned memory** is locked in physical RAM - no paging allowed.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Pageable Memory (default)     â”‚  Pinned Memory             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  CPU Memory                    â”‚  CPU Memory (locked)       â”‚\n",
    "â”‚      â†“ (copy to staging)       â”‚      â†“ (direct DMA)        â”‚\n",
    "â”‚  Pinned Buffer                 â”‚                            â”‚\n",
    "â”‚      â†“ (DMA to GPU)            â”‚      â†“ (DMA to GPU)        â”‚\n",
    "â”‚  GPU Memory                    â”‚  GPU Memory                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  TWO copies!                   â”‚  ONE copy! (faster)        â”‚\n",
    "â”‚  Can be swapped to disk        â”‚  Always in RAM             â”‚\n",
    "â”‚  Unlimited size                â”‚  Limited by system RAM     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f57309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pinned_vs_pageable(n):\n",
    "    \"\"\"Compare transfer speeds with pinned vs pageable memory\"\"\"\n",
    "    print(f\"\\nğŸ“Š Pinned vs Pageable Memory (N = {n:,})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Pageable memory (default NumPy allocation)\n",
    "    pageable = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Pinned memory\n",
    "    pinned = cuda.pinned_array(n, dtype=np.float32)\n",
    "    pinned[:] = pageable  # Copy data to pinned\n",
    "    \n",
    "    # Measure pageable transfer\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        d_pageable = cuda.to_device(pageable)\n",
    "        cuda.synchronize()\n",
    "    pageable_time = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Measure pinned transfer\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        d_pinned = cuda.to_device(pinned)\n",
    "        cuda.synchronize()\n",
    "    pinned_time = (time.perf_counter() - start) / 10\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    data_size_gb = n * 4 / 1e9  # float32 = 4 bytes\n",
    "    pageable_bw = data_size_gb / pageable_time\n",
    "    pinned_bw = data_size_gb / pinned_time\n",
    "    \n",
    "    print(f\"  Pageable: {pageable_time*1000:.3f} ms ({pageable_bw:.2f} GB/s)\")\n",
    "    print(f\"  Pinned:   {pinned_time*1000:.3f} ms ({pinned_bw:.2f} GB/s)\")\n",
    "    print(f\"  Speedup:  {pageable_time/pinned_time:.2f}x\")\n",
    "\n",
    "# Compare for different sizes\n",
    "for size in [1_000_000, 10_000_000, 50_000_000]:\n",
    "    compare_pinned_vs_pageable(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3578ad8",
   "metadata": {},
   "source": [
    "## 5. When Transfer Time Dominates\n",
    "\n",
    "For simple operations like vector addition, **memory transfers dominate execution time**.\n",
    "\n",
    "This is why:\n",
    "1. GPU is best for **compute-intensive** operations\n",
    "2. You should **minimize transfers** (keep data on GPU)\n",
    "3. **Batch operations** together before copying back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transfer_overhead(n):\n",
    "    \"\"\"Show how transfer time compares to compute time\"\"\"\n",
    "    print(f\"\\nğŸ“Š Transfer vs Compute Analysis (N = {n:,})\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    a = np.random.randn(n).astype(np.float32)\n",
    "    b = np.random.randn(n).astype(np.float32)\n",
    "    \n",
    "    # Measure H2D transfer\n",
    "    start = time.perf_counter()\n",
    "    a_d = cuda.to_device(a)\n",
    "    b_d = cuda.to_device(b)\n",
    "    c_d = cuda.device_array(n, dtype=np.float32)\n",
    "    cuda.synchronize()\n",
    "    h2d_time = time.perf_counter() - start\n",
    "    \n",
    "    # Measure kernel (data already on GPU)\n",
    "    threads = 256\n",
    "    blocks = math.ceil(n / threads)\n",
    "    \n",
    "    @cuda.jit\n",
    "    def add_kernel(a, b, c):\n",
    "        idx = cuda.grid(1)\n",
    "        if idx < c.size:\n",
    "            c[idx] = a[idx] + b[idx]\n",
    "    \n",
    "    # Warmup\n",
    "    add_kernel[blocks, threads](a_d, b_d, c_d)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        add_kernel[blocks, threads](a_d, b_d, c_d)\n",
    "    cuda.synchronize()\n",
    "    kernel_time = (time.perf_counter() - start) / 100\n",
    "    \n",
    "    # Measure D2H transfer\n",
    "    start = time.perf_counter()\n",
    "    c = c_d.copy_to_host()\n",
    "    cuda.synchronize()\n",
    "    d2h_time = time.perf_counter() - start\n",
    "    \n",
    "    total = h2d_time + kernel_time + d2h_time\n",
    "    \n",
    "    print(f\"  H2D Transfer:  {h2d_time*1000:>8.3f} ms ({h2d_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  Kernel:        {kernel_time*1000:>8.3f} ms ({kernel_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  D2H Transfer:  {d2h_time*1000:>8.3f} ms ({d2h_time/total*100:>5.1f}%)\")\n",
    "    print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(f\"  Total:         {total*1000:>8.3f} ms\")\n",
    "    \n",
    "    if (h2d_time + d2h_time) > kernel_time:\n",
    "        print(f\"\\n  âš ï¸  Transfer time ({(h2d_time + d2h_time)*1000:.2f} ms) > Kernel time ({kernel_time*1000:.2f} ms)\")\n",
    "        print(f\"     Consider: Keep data on GPU, batch operations!\")\n",
    "\n",
    "analyze_transfer_overhead(10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87ad6d",
   "metadata": {},
   "source": [
    "## ğŸ¯ Exercises\n",
    "\n",
    "### Exercise 1: Memory Reuse Pattern\n",
    "Implement a pipeline that reuses GPU memory for multiple operations without copying back to CPU between each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62579f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 1: Memory Reuse Pattern\n",
    "# Compute: result = ((a + b) * c) - d\n",
    "# Do this with ONE H2D transfer and ONE D2H transfer\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] + b[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def mul_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] * b[idx]\n",
    "\n",
    "@cuda.jit  \n",
    "def sub_kernel(a, b, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < out.size:\n",
    "        out[idx] = a[idx] - b[idx]\n",
    "\n",
    "def pipeline_inefficient(a, b, c, d):\n",
    "    \"\"\"BAD: Transfers after each operation\"\"\"\n",
    "    # TODO: This is the slow way - see how many transfers happen\n",
    "    pass\n",
    "\n",
    "def pipeline_efficient(a, b, c, d):\n",
    "    \"\"\"GOOD: All operations on GPU, one final transfer\"\"\"\n",
    "    # TODO: Implement efficient version\n",
    "    # 1. Copy all inputs to GPU\n",
    "    # 2. Perform all operations\n",
    "    # 3. Copy only result back\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "# N = 10_000_000\n",
    "# a = np.random.randn(N).astype(np.float32)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bb032",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory Bandwidth Calculation\n",
    "Calculate the theoretical vs achieved memory bandwidth for a copy operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Exercise 2: Bandwidth Measurement\n",
    "@cuda.jit\n",
    "def copy_kernel(src, dst):\n",
    "    \"\"\"Simple copy kernel - measures GPU memory bandwidth\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < dst.size:\n",
    "        dst[idx] = src[idx]\n",
    "\n",
    "def measure_bandwidth(n):\n",
    "    \"\"\"Measure achieved memory bandwidth\"\"\"\n",
    "    # TODO: \n",
    "    # 1. Create two device arrays\n",
    "    # 2. Time the copy kernel\n",
    "    # 3. Calculate bandwidth: (bytes_read + bytes_written) / time\n",
    "    # 4. Compare to theoretical peak (from device properties)\n",
    "    pass\n",
    "\n",
    "# measure_bandwidth(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae5362",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Memory Management Summary:\n",
    "\n",
    "| Method | When to Use | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **Explicit** (`to_device`, `copy_to_host`) | Production code, max control | Fastest, predictable | More code |\n",
    "| **Unified** (`managed_array`) | Prototyping, complex access patterns | Simple code | Page fault overhead |\n",
    "| **Pinned** (`pinned_array`) | High-bandwidth transfers | ~2x transfer speed | Uses system RAM |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Minimize transfers**: Keep data on GPU as long as possible\n",
    "2. **Use pinned memory**: For frequent large transfers\n",
    "3. **Batch operations**: Don't copy back between every operation\n",
    "4. **Profile first**: Measure before optimizing\n",
    "\n",
    "### Memory Bandwidth Rule of Thumb:\n",
    "- PCIe 3.0 x16: ~16 GB/s\n",
    "- PCIe 4.0 x16: ~32 GB/s  \n",
    "- GPU Memory: 200-900 GB/s (much faster!)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Next Up: Day 4 - Error Handling & Debugging\n",
    "- CUDA error codes\n",
    "- Debugging techniques\n",
    "- Common pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”— Resources\n",
    "- [CUDA Memory Guide](../../cuda-programming-guide/02-basics/cuda-memory.md)\n",
    "- [Understanding Memory](../../cuda-programming-guide/02-basics/understanding-memory.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
