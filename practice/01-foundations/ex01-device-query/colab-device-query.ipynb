{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1fcf9d",
   "metadata": {},
   "source": [
    "# Exercise 01: Device Query\n",
    "\n",
    "Query and display GPU properties to understand your hardware.\n",
    "\n",
    "## Learning Goals\n",
    "- Use `cudaGetDeviceCount()` and `cudaGetDeviceProperties()`\n",
    "- Understand key GPU specifications\n",
    "- First successful CUDA program compilation\n",
    "\n",
    "## üöÄ Setup Instructions\n",
    "\n",
    "**IMPORTANT**: Enable GPU before starting!\n",
    "- Go to: **Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6dde0d",
   "metadata": {},
   "source": [
    "## Step 1: Verify CUDA Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49388c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version and GPU availability\n",
    "!nvcc --version\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84568717",
   "metadata": {},
   "source": [
    "## Step 2: Your Exercise - Complete the TODOs\n",
    "\n",
    "Below is the starter code with TODOs. Your task:\n",
    "1. Read through the code\n",
    "2. Complete the sections marked with `TODO`\n",
    "3. Run the code to see your GPU properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile device_query.cu\n",
    "/**\n",
    " * Exercise 01: Device Query\n",
    " * \n",
    " * Query and display GPU properties.\n",
    " * \n",
    " * TODO: Complete the missing parts marked with TODO\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Error checking macro\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n",
    "                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "int main() {\n",
    "    int deviceCount = 0;\n",
    "    \n",
    "    // TODO 1: Get the number of CUDA devices\n",
    "    // Hint: Use cudaGetDeviceCount()\n",
    "    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n",
    "    \n",
    "    if (deviceCount == 0) {\n",
    "        printf(\"No CUDA-capable devices found!\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    printf(\"Found %d CUDA device(s)\\n\\n\", deviceCount);\n",
    "    \n",
    "    // Query properties for each device\n",
    "    for (int dev = 0; dev < deviceCount; dev++) {\n",
    "        cudaDeviceProp prop;\n",
    "        \n",
    "        // TODO 2: Get device properties\n",
    "        // Hint: Use cudaGetDeviceProperties()\n",
    "        CUDA_CHECK(cudaGetDeviceProperties(&prop, dev));\n",
    "        \n",
    "        // Display basic information\n",
    "        printf(\"========== Device %d: %s ==========\\n\", dev, prop.name);\n",
    "        printf(\"  Compute Capability:        %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"  Total Global Memory:       %.2f GB\\n\", \n",
    "               prop.totalGlobalMem / 1e9);\n",
    "        \n",
    "        // TODO 3: Display multiprocessor count\n",
    "        // Hint: prop.multiProcessorCount\n",
    "        printf(\"  Multiprocessors (SMs):     %d\\n\", prop.multiProcessorCount);\n",
    "        \n",
    "        // TODO 4: Display CUDA cores per SM (depends on compute capability)\n",
    "        int coresPerSM = 0;\n",
    "        switch (prop.major) {\n",
    "            case 2:  coresPerSM = 32; break;  // Fermi\n",
    "            case 3:  coresPerSM = 192; break; // Kepler\n",
    "            case 5:  coresPerSM = 128; break; // Maxwell\n",
    "            case 6:  coresPerSM = (prop.minor == 0) ? 64 : 128; break; // Pascal\n",
    "            case 7:  coresPerSM = 64; break;  // Volta/Turing\n",
    "            case 8:  coresPerSM = (prop.minor == 0) ? 64 : 128; break; // Ampere\n",
    "            default: coresPerSM = 0;\n",
    "        }\n",
    "        printf(\"  CUDA Cores per SM:         %d\\n\", coresPerSM);\n",
    "        printf(\"  Total CUDA Cores:          %d\\n\", \n",
    "               prop.multiProcessorCount * coresPerSM);\n",
    "        \n",
    "        // TODO 5: Display memory information\n",
    "        printf(\"  Shared Memory per Block:   %zu KB\\n\", \n",
    "               prop.sharedMemPerBlock / 1024);\n",
    "        printf(\"  Registers per Block:       %d\\n\", prop.regsPerBlock);\n",
    "        \n",
    "        // TODO 6: Display thread limits\n",
    "        printf(\"  Warp Size:                 %d\\n\", prop.warpSize);\n",
    "        printf(\"  Max Threads per Block:     %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  Max Block Dimensions:      (%d, %d, %d)\\n\",\n",
    "               prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]);\n",
    "        printf(\"  Max Grid Dimensions:       (%d, %d, %d)\\n\",\n",
    "               prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]);\n",
    "        \n",
    "        // TODO 7: Display clock speeds\n",
    "        printf(\"  GPU Clock Rate:            %.2f GHz\\n\", \n",
    "               prop.clockRate / 1e6);\n",
    "        printf(\"  Memory Clock Rate:         %.2f GHz\\n\",\n",
    "               prop.memoryClockRate / 1e6);\n",
    "        printf(\"  Memory Bus Width:          %d-bit\\n\", prop.memoryBusWidth);\n",
    "        \n",
    "        // TODO 8: Calculate and display memory bandwidth\n",
    "        double memBandwidth = 2.0 * prop.memoryClockRate * \n",
    "                             (prop.memoryBusWidth / 8.0) / 1e6;\n",
    "        printf(\"  Peak Memory Bandwidth:     %.2f GB/s\\n\", memBandwidth);\n",
    "        \n",
    "        // TODO 9: Calculate theoretical peak performance\n",
    "        // Formula: CUDA Cores √ó Clock Speed √ó 2 (FMA operations)\n",
    "        int totalCores = prop.multiProcessorCount * coresPerSM;\n",
    "        double peakGFLOPS = (totalCores * (prop.clockRate / 1e6) * 2.0);\n",
    "        printf(\"  Peak FP32 Performance:     %.2f TFLOPS\\n\", peakGFLOPS / 1000.0);\n",
    "        \n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25be7d",
   "metadata": {},
   "source": [
    "## Step 3: Compile the Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with nvcc\n",
    "!nvcc -arch=sm_75 device_query.cu -o device_query\n",
    "print(\"‚úÖ Compilation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b397c",
   "metadata": {},
   "source": [
    "## Step 4: Run and See Your GPU Properties!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fca1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the compiled program\n",
    "!./device_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df512a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Python Comparison (Optional)\n",
    "\n",
    "Here's the same device query in Python using Numba. Compare with the C++ version above!\n",
    "\n",
    "### C++ vs Python Syntax\n",
    "\n",
    "| Task | CUDA C++ | Python (Numba) |\n",
    "|------|----------|----------------|\n",
    "| Get device count | `cudaGetDeviceCount(&count)` | `len(cuda.gpus)` |\n",
    "| Get device | `cudaGetDeviceProperties(&prop, 0)` | `cuda.get_current_device()` |\n",
    "| Device name | `prop.name` | `device.name` |\n",
    "| Compute capability | `prop.major, prop.minor` | `device.compute_capability` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêç Python Version - Device Query with Numba\n",
    "!pip install numba -q\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PYTHON (Numba) DEVICE QUERY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check CUDA availability\n",
    "if cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA is available!\")\n",
    "    print(f\"   Number of GPUs: {len(cuda.gpus)}\")\n",
    "    \n",
    "    # Get current device\n",
    "    device = cuda.get_current_device()\n",
    "    \n",
    "    print(f\"\\nüìä Device: {device.name.decode()}\")\n",
    "    print(f\"   Compute Capability: {device.compute_capability}\")\n",
    "    \n",
    "    # Get more properties\n",
    "    ctx = cuda.current_context()\n",
    "    \n",
    "    # Memory info\n",
    "    free_mem, total_mem = cuda.current_context().get_memory_info()\n",
    "    print(f\"   Total Memory: {total_mem / 1e9:.2f} GB\")\n",
    "    print(f\"   Free Memory: {free_mem / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Thread limits (from device attributes)\n",
    "    print(f\"   Max Threads per Block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "    print(f\"   Max Block Dimensions: {device.MAX_BLOCK_DIM_X} x {device.MAX_BLOCK_DIM_Y} x {device.MAX_BLOCK_DIM_Z}\")\n",
    "    print(f\"   Max Grid Dimensions: {device.MAX_GRID_DIM_X} x {device.MAX_GRID_DIM_Y} x {device.MAX_GRID_DIM_Z}\")\n",
    "    print(f\"   Warp Size: {device.WARP_SIZE}\")\n",
    "    print(f\"   Multiprocessors: {device.MULTIPROCESSOR_COUNT}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is not available!\")\n",
    "\n",
    "print(\"\\nüí° Notice: Python gives less detail than C++ cudaGetDeviceProperties()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83171fd1",
   "metadata": {},
   "source": [
    "### üéØ Key Takeaway\n",
    "\n",
    "**C++ provides full access** to all GPU properties via `cudaDeviceProp` (50+ fields).\n",
    "\n",
    "**Python (Numba)** gives you the essentials, but less detail.\n",
    "\n",
    "For serious CUDA work ‚Üí **Stick with C++** (the exercise above)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3a60f8",
   "metadata": {},
   "source": [
    "## üìä Understanding the Output\n",
    "\n",
    "### Key Properties Explained\n",
    "\n",
    "| Property | What it means | Why it matters |\n",
    "|----------|---------------|----------------|\n",
    "| **Compute Capability** | GPU architecture version (e.g., 7.5 for T4) | Determines supported CUDA features |\n",
    "| **Multiprocessors (SMs)** | Number of parallel processing units | More SMs = more parallelism |\n",
    "| **CUDA Cores** | Processing elements that execute instructions | More cores = higher throughput |\n",
    "| **Global Memory** | Total GPU memory (usually 16GB on T4) | Limits problem size you can solve |\n",
    "| **Shared Memory** | Fast on-chip memory per block | Critical for performance optimization |\n",
    "| **Max Threads/Block** | Thread limit per block (usually 1024) | Affects kernel launch configuration |\n",
    "| **Warp Size** | Threads executed together (always 32) | SIMT execution model |\n",
    "| **Memory Bandwidth** | Data transfer rate (GB/s) | Often the bottleneck in GPU code |\n",
    "| **Peak FLOPS** | Theoretical floating-point operations/sec | Maximum compute performance |\n",
    "\n",
    "### For T4 GPU (typical Colab)\n",
    "- Compute Capability: **7.5**\n",
    "- CUDA Cores: **2560**\n",
    "- Memory: **16 GB**\n",
    "- Bandwidth: **300 GB/s**\n",
    "- Peak FP32: **~8.1 TFLOPS**\n",
    "\n",
    "### üéØ Tasks\n",
    "\n",
    "- ‚úÖ Complete all TODOs in the code\n",
    "- ‚úÖ Run and understand the output\n",
    "- ‚úÖ Compare memory bandwidth with peak FLOPS\n",
    "- ‚úÖ Calculate: How many GB can you transfer in 1ms?\n",
    "- ‚úÖ Calculate: How many float operations in 1ms?\n",
    "\n",
    "### üí° Bonus Exercises\n",
    "\n",
    "1. **Arithmetic Intensity**: If you need to do 1000 operations per byte transferred, will you be memory-bound or compute-bound?\n",
    "2. **Thread Blocks**: If your kernel uses 512 threads per block, how many blocks can fit on one SM? (Hint: check `maxThreadsPerMultiProcessor`)\n",
    "3. **Memory Math**: At peak bandwidth, how long does it take to fill all GPU memory?\n",
    "\n",
    "### üìö Learn More\n",
    "\n",
    "- [CUDA C Programming Guide - Compute Capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)\n",
    "- [T4 GPU Specifications](https://www.nvidia.com/en-us/data-center/tesla-t4/)\n",
    "- [Understanding GPU Architecture](../../../cuda-programming-guide/01-introduction/hardware-implementation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc9b09",
   "metadata": {},
   "source": [
    "## üéì Solution\n",
    "\n",
    "If you completed the exercise, compare your implementation with the solution below.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solution notes</summary>\n",
    "\n",
    "### Key Points\n",
    "1. **Error Checking**: Always use `CUDA_CHECK()` macro for CUDA API calls\n",
    "2. **Device Count**: `cudaGetDeviceCount()` returns number of available GPUs\n",
    "3. **Properties**: `cudaGetDeviceProperties()` fills a `cudaDeviceProp` structure\n",
    "4. **CUDA Cores**: Calculation depends on compute capability (architecture)\n",
    "5. **Bandwidth**: Formula is `2 √ó memory_clock √ó (bus_width/8) / 1e6`\n",
    "6. **Peak FLOPS**: `cores √ó clock √ó 2` (2 for FMA - fused multiply-add)\n",
    "\n",
    "All TODOs in the code above are already completed as hints!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdbd68",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è Next Exercise\n",
    "\n",
    "Continue to [Exercise 02: Hello GPU](../ex02-hello-gpu/colab-hello-gpu.ipynb) to write your first kernel!\n",
    "\n",
    "---\n",
    "\n",
    "**Completed this exercise?** Great! Save this notebook to your Drive and move on to the next one."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
