{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55eb383f",
   "metadata": {},
   "source": [
    "# Exercise 02: Hello GPU üëã\n",
    "\n",
    "Write your first CUDA kernel - printing from the GPU!\n",
    "\n",
    "## Learning Goals\n",
    "- Write a `__global__` function (kernel)\n",
    "- Launch a kernel with `<<<blocks, threads>>>`\n",
    "- Understand thread/block indexing\n",
    "- See parallel execution in action\n",
    "\n",
    "## üöÄ Setup\n",
    "\n",
    "**Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724167ae",
   "metadata": {},
   "source": [
    "## Step 1: Verify CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6162ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0554dc4",
   "metadata": {},
   "source": [
    "## üìö Key Concepts\n",
    "\n",
    "### Kernel Declaration\n",
    "```cpp\n",
    "__global__ void myKernel() {\n",
    "    // This runs on GPU in parallel!\n",
    "}\n",
    "```\n",
    "\n",
    "### Kernel Launch Syntax\n",
    "```cpp\n",
    "myKernel<<<numBlocks, threadsPerBlock>>>();\n",
    "cudaDeviceSynchronize();  // Wait for GPU to finish\n",
    "```\n",
    "\n",
    "### Thread Identification\n",
    "```cpp\n",
    "int threadId = threadIdx.x;   // 0 to blockDim.x-1\n",
    "int blockId = blockIdx.x;     // 0 to gridDim.x-1\n",
    "int globalId = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "\n",
    "### Visual Example\n",
    "```\n",
    "Launch: <<<2, 4>>>\n",
    "\n",
    "Block 0:          Block 1:\n",
    "[T0 T1 T2 T3]    [T0 T1 T2 T3]\n",
    "Global IDs:      Global IDs:\n",
    "[0  1  2  3]     [4  5  6  7]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e282dfb",
   "metadata": {},
   "source": [
    "## Step 2: Your Exercise - Complete the Kernel\n",
    "\n",
    "Fill in the TODOs below to create your first working kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hello_gpu.cu\n",
    "/**\n",
    " * Exercise 02: Hello GPU\n",
    " * \n",
    " * Your first CUDA kernel!\n",
    " * \n",
    " * TODO: Complete the kernel and launch configurations\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// TODO 1: Write a kernel that prints a greeting from each thread\n",
    "// The kernel should print:\n",
    "// \"Hello from block X, thread Y (global ID: Z)\"\n",
    "// \n",
    "// Hints:\n",
    "// - Use printf() - it works on GPU!\n",
    "// - blockIdx.x gives block index\n",
    "// - threadIdx.x gives thread index within block\n",
    "// - Global ID = blockIdx.x * blockDim.x + threadIdx.x\n",
    "\n",
    "__global__ void helloKernel() {\n",
    "    // Calculate global thread ID\n",
    "    int globalId = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Print greeting from this thread\n",
    "    printf(\"Hello from block %d, thread %d (global ID: %d)\\n\",\n",
    "           blockIdx.x, threadIdx.x, globalId);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"=== Launching with 1 block, 8 threads ===\\n\");\n",
    "    \n",
    "    // TODO 2: Launch helloKernel with 1 block and 8 threads\n",
    "    // Syntax: kernelName<<<numBlocks, threadsPerBlock>>>();\n",
    "    helloKernel<<<1, 8>>>();\n",
    "    \n",
    "    // Don't forget to synchronize!\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\n=== Launching with 2 blocks, 4 threads each ===\\n\");\n",
    "    \n",
    "    // TODO 3: Launch helloKernel with 2 blocks and 4 threads per block\n",
    "    helloKernel<<<2, 4>>>();\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\n=== Launching with 4 blocks, 2 threads each ===\\n\");\n",
    "    \n",
    "    // TODO 4: Launch helloKernel with 4 blocks and 2 threads per block\n",
    "    helloKernel<<<4, 2>>>();\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\n‚úÖ All kernels completed!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448d17c",
   "metadata": {},
   "source": [
    "## Step 3: Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 hello_gpu.cu -o hello_gpu\n",
    "print(\"‚úÖ Compilation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66c2c9",
   "metadata": {},
   "source": [
    "## Step 4: Run and Observe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./hello_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140b12a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Python Comparison (Optional)\n",
    "\n",
    "Want to see the same kernel in Python? Here's how it looks using **Numba CUDA**.\n",
    "\n",
    "> **Note**: C++ is the industry standard. Python is great for quick prototyping and learning concepts.\n",
    "\n",
    "### Side-by-Side Comparison\n",
    "\n",
    "| CUDA C++ | Python (Numba) |\n",
    "|----------|----------------|\n",
    "| `__global__ void kernel()` | `@cuda.jit` |\n",
    "| `blockIdx.x` | `cuda.blockIdx.x` |\n",
    "| `threadIdx.x` | `cuda.threadIdx.x` |\n",
    "| `blockDim.x` | `cuda.blockDim.x` |\n",
    "| `<<<blocks, threads>>>` | `kernel[blocks, threads]()` |\n",
    "| `cudaDeviceSynchronize()` | `cuda.synchronize()` |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "| Use C++ When... | Use Python When... |\n",
    "|-----------------|-------------------|\n",
    "| Production code | Quick prototyping |\n",
    "| Maximum performance | Data science workflows |\n",
    "| Low-level control needed | Learning concepts |\n",
    "| Industry/job requirements | Rapid experimentation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêç Python Version (Numba) - Same logic, different syntax\n",
    "# Run this cell to see the Python equivalent!\n",
    "\n",
    "!pip install numba -q\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Python kernel - note the @cuda.jit decorator instead of __global__\n",
    "@cuda.jit\n",
    "def hello_kernel_python():\n",
    "    # Same indexing, just with cuda. prefix\n",
    "    block_id = cuda.blockIdx.x\n",
    "    thread_id = cuda.threadIdx.x\n",
    "    global_id = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    # Note: printf doesn't work in Numba, we'd need to write to an array\n",
    "    # This is one limitation of Python CUDA\n",
    "\n",
    "# To actually see output, we need to write to memory (Python limitation)\n",
    "@cuda.jit\n",
    "def hello_kernel_with_output(output):\n",
    "    global_id = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if global_id < output.size:\n",
    "        output[global_id] = global_id  # Store the global ID\n",
    "\n",
    "# Launch equivalent to <<<1, 8>>>\n",
    "output = np.zeros(8, dtype=np.int32)\n",
    "d_output = cuda.to_device(output)\n",
    "\n",
    "hello_kernel_with_output[1, 8](d_output)  # [blocks, threads] syntax\n",
    "cuda.synchronize()\n",
    "\n",
    "result = d_output.copy_to_host()\n",
    "print(\"Python Numba result (global IDs):\", result)\n",
    "print(\"\\nüí° Notice: Python syntax [1, 8] vs C++ syntax <<<1, 8>>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c683ea",
   "metadata": {},
   "source": [
    "### üéØ Key Takeaway\n",
    "\n",
    "**C++ gives you more control** (printf from GPU, full CUDA API), while **Python is more concise** but has limitations.\n",
    "\n",
    "For serious CUDA work ‚Üí **Stick with C++** (the exercises above)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f2c78",
   "metadata": {},
   "source": [
    "## üîç Observations & Questions\n",
    "\n",
    "### What to Notice:\n",
    "\n",
    "1. **Output Order**: Are the threads printed in order?\n",
    "   - ‚ùì Why might they appear out of order?\n",
    "   - üí° Threads execute in parallel - no guaranteed order!\n",
    "\n",
    "2. **Block vs Thread IDs**:\n",
    "   - In launch `<<<1, 8>>>`: How many blocks? How many threads?\n",
    "   - In launch `<<<2, 4>>>`: How are global IDs calculated?\n",
    "\n",
    "3. **Total Threads**:\n",
    "   - `<<<1, 8>>>` = 1 √ó 8 = **8 threads**\n",
    "   - `<<<2, 4>>>` = 2 √ó 4 = **8 threads** (same total, different organization!)\n",
    "   - `<<<4, 2>>>` = 4 √ó 2 = **8 threads**\n",
    "\n",
    "### üß™ Experiments to Try\n",
    "\n",
    "Run these experiments by modifying the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile experiment1.cu\n",
    "// Experiment 1: Large number of threads\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void helloKernel() {\n",
    "    int globalId = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    printf(\"Thread %d\\n\", globalId);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching 256 threads...\\n\");\n",
    "    helloKernel<<<4, 64>>>();  // 4 blocks √ó 64 threads = 256 threads\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 experiment1.cu -o experiment1 && ./experiment1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a9a49",
   "metadata": {},
   "source": [
    "## üìä Understanding Kernel Launch\n",
    "\n",
    "### Launch Configuration: `<<<blocks, threads>>>`\n",
    "\n",
    "| Syntax | Meaning | Total Threads |\n",
    "|--------|---------|---------------|\n",
    "| `<<<1, 256>>>` | 1 block, 256 threads/block | 256 |\n",
    "| `<<<256, 1>>>` | 256 blocks, 1 thread/block | 256 |\n",
    "| `<<<16, 16>>>` | 16 blocks, 16 threads/block | 256 |\n",
    "| `<<<4, 64>>>` | 4 blocks, 64 threads/block | 256 |\n",
    "\n",
    "**All launch the same total number of threads, but organized differently!**\n",
    "\n",
    "### Why Does Organization Matter?\n",
    "\n",
    "1. **Hardware Limits**:\n",
    "   - Max threads per block: **1024** (on most GPUs)\n",
    "   - Must stay within limits!\n",
    "\n",
    "2. **Performance**:\n",
    "   - Threads in same block can share memory\n",
    "   - Better organization = better performance\n",
    "\n",
    "3. **Problem Mapping**:\n",
    "   - For 2D image: `<<<dim3(width/16, height/16), dim3(16, 16)>>>`\n",
    "   - Maps naturally to problem structure\n",
    "\n",
    "### üéØ Tasks Checklist\n",
    "\n",
    "- ‚úÖ Complete the `helloKernel()` function\n",
    "- ‚úÖ Launch with different block/thread configurations\n",
    "- ‚úÖ Observe non-deterministic output order\n",
    "- ‚úÖ Calculate global IDs correctly\n",
    "- ‚úÖ Try launching 1024 threads (max per block)\n",
    "- ‚úÖ Try launching 10,000 threads across multiple blocks\n",
    "\n",
    "### üí° Bonus Challenges\n",
    "\n",
    "1. **Max Threads**: Launch the maximum threads per block (1024)\n",
    "2. **Huge Launch**: Launch 1 million threads. How many blocks do you need?\n",
    "3. **Warp Alignment**: Launch 32, 64, 96 threads. Notice patterns?\n",
    "4. **Error Check**: What happens if you try `<<<1, 2048>>>`? (exceeds limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e88f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bonus_max.cu\n",
    "// Bonus: Maximum threads per block\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void maxThreadsKernel() {\n",
    "    int globalId = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (globalId % 100 == 0) {  // Print every 100th to avoid spam\n",
    "        printf(\"Thread %d\\n\", globalId);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching 1024 threads per block (max)...\\n\");\n",
    "    maxThreadsKernel<<<1, 1024>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"\\nLaunching 1 million threads...\\n\");\n",
    "    // Need: 1,000,000 / 1024 ‚âà 977 blocks\n",
    "    int numBlocks = (1000000 + 1024 - 1) / 1024;  // Ceiling division\n",
    "    maxThreadsKernel<<<numBlocks, 1024>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"‚úÖ Launched %d threads!\\n\", numBlocks * 1024);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 bonus_max.cu -o bonus_max && ./bonus_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb56a3",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "1. **printf from GPU**:\n",
    "   - Requires compute capability 2.0+ ‚úÖ\n",
    "   - Output buffer is limited (~1MB)\n",
    "   - Too many printf calls may lose output\n",
    "\n",
    "2. **cudaDeviceSynchronize()**:\n",
    "   - **Essential** to see printf output\n",
    "   - Waits for all GPU threads to finish\n",
    "   - Without it, program may exit before GPU prints\n",
    "\n",
    "3. **Non-deterministic Order**:\n",
    "   - Threads execute in parallel\n",
    "   - No guaranteed execution order\n",
    "   - **Never assume sequential order!**\n",
    "\n",
    "4. **Thread Limits**:\n",
    "   - Max threads per block: 1024\n",
    "   - Max blocks: 2^31 - 1 (huge!)\n",
    "   - Check limits with `cudaDeviceProp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d2254",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "You've learned:\n",
    "- ‚úÖ How to write a `__global__` kernel\n",
    "- ‚úÖ How to launch kernels with `<<<blocks, threads>>>`\n",
    "- ‚úÖ Thread and block indexing\n",
    "- ‚úÖ Parallel execution is non-deterministic\n",
    "\n",
    "**Next**: Learn how to do actual computation on GPU!\n",
    "- Vector addition\n",
    "- Memory transfers (host ‚Üî device)\n",
    "- Array processing\n",
    "\n",
    "Continue to the next exercise or explore [Week 1 Learning Path](../../../learning-path/week-01/)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions to Think About:**\n",
    "1. Why do we need blocks AND threads?\n",
    "2. What happens if we launch more threads than CUDA cores?\n",
    "3. How does the GPU schedule thread execution?\n",
    "\n",
    "Answers in the [Programming Model](../../../cuda-programming-guide/01-introduction/programming-model.md) guide!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
