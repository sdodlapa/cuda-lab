{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a242347",
   "metadata": {},
   "source": [
    "# Exercise 03: Vector Addition üßÆ\n",
    "\n",
    "Your first real computation on the GPU - adding two arrays!\n",
    "\n",
    "## Learning Goals\n",
    "- Pass data to and from the GPU (`cudaMalloc`, `cudaMemcpy`)\n",
    "- Calculate global thread index\n",
    "- Handle arrays larger than thread count\n",
    "- Verify GPU results\n",
    "\n",
    "## üöÄ Setup\n",
    "\n",
    "**Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e429696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA\n",
    "!nvcc --version\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ea2d4",
   "metadata": {},
   "source": [
    "## üìö Key Concepts\n",
    "\n",
    "### Memory Management Pattern\n",
    "```cpp\n",
    "// 1. Allocate on GPU\n",
    "float *d_array;\n",
    "cudaMalloc(&d_array, size);\n",
    "\n",
    "// 2. Copy data TO GPU\n",
    "cudaMemcpy(d_array, h_array, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "// 3. Run kernel\n",
    "kernel<<<blocks, threads>>>(d_array);\n",
    "\n",
    "// 4. Copy result FROM GPU\n",
    "cudaMemcpy(h_result, d_result, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "// 5. Free GPU memory\n",
    "cudaFree(d_array);\n",
    "```\n",
    "\n",
    "### Grid/Block Calculation\n",
    "```cpp\n",
    "int threadsPerBlock = 256;\n",
    "int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;  // Ceiling division\n",
    "```\n",
    "\n",
    "### Global Thread Index\n",
    "```cpp\n",
    "int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "if (i < n) {  // Bounds check!\n",
    "    c[i] = a[i] + b[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8277c5e",
   "metadata": {},
   "source": [
    "## Step 1: Complete the Exercise\n",
    "\n",
    "Fill in the TODOs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_add.cu\n",
    "/**\n",
    " * Exercise 03: Vector Addition\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n",
    "                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "// TODO 1: Write the vector addition kernel\n",
    "// Each thread adds one element: c[i] = a[i] + b[i]\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    // Calculate global thread index\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Check bounds and perform addition\n",
    "    if (i < n) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;  // 1 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    printf(\"Vector Addition: %d elements\\n\", n);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize input arrays\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = 1.0f;\n",
    "        h_b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // TODO 2: Allocate device memory\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    CUDA_CHECK(cudaMalloc(&d_a, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_b, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_c, size));\n",
    "    \n",
    "    // TODO 3: Copy input data to device\n",
    "    CUDA_CHECK(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // TODO 4: Calculate grid dimensions and launch kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    printf(\"Launching: %d blocks, %d threads/block\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    \n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    // TODO 5: Copy result back to host\n",
    "    CUDA_CHECK(cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify result\n",
    "    bool success = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_c[i] != 3.0f) {\n",
    "            printf(\"Error at %d: expected 3.0, got %.2f\\n\", i, h_c[i]);\n",
    "            success = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"%s\\n\", success ? \"‚úÖ PASSED!\" : \"‚ùå FAILED!\");\n",
    "    \n",
    "    // TODO 6: Free device memory\n",
    "    CUDA_CHECK(cudaFree(d_a));\n",
    "    CUDA_CHECK(cudaFree(d_b));\n",
    "    CUDA_CHECK(cudaFree(d_c));\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "!nvcc -arch=sm_75 vector_add.cu -o vector_add\n",
    "print(\"‚úÖ Compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5105253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "!./vector_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b44067",
   "metadata": {},
   "source": [
    "## üß™ Experiments\n",
    "\n",
    "Try these variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ba85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_add_timed.cu\n",
    "// Timed version - compare GPU vs CPU\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <time.h>\n",
    "\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) c[i] = a[i] + b[i];\n",
    "}\n",
    "\n",
    "void vectorAddCPU(float *a, float *b, float *c, int n) {\n",
    "    for (int i = 0; i < n; i++) c[i] = a[i] + b[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) { h_a[i] = 1.0f; h_b[i] = 2.0f; }\n",
    "    \n",
    "    // CPU timing\n",
    "    clock_t start = clock();\n",
    "    vectorAddCPU(h_a, h_b, h_c, n);\n",
    "    double cpu_time = (double)(clock() - start) / CLOCKS_PER_SEC * 1000;\n",
    "    printf(\"CPU time: %.3f ms\\n\", cpu_time);\n",
    "    \n",
    "    // GPU timing (including transfers)\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaEvent_t start_gpu, stop_gpu;\n",
    "    cudaEventCreate(&start_gpu);\n",
    "    cudaEventCreate(&stop_gpu);\n",
    "    \n",
    "    cudaEventRecord(start_gpu);\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    vectorAdd<<<(n+255)/256, 256>>>(d_a, d_b, d_c, n);\n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    cudaEventRecord(stop_gpu);\n",
    "    cudaEventSynchronize(stop_gpu);\n",
    "    \n",
    "    float gpu_time;\n",
    "    cudaEventElapsedTime(&gpu_time, start_gpu, stop_gpu);\n",
    "    printf(\"GPU time (with transfers): %.3f ms\\n\", gpu_time);\n",
    "    printf(\"Speedup: %.2fx\\n\", cpu_time / gpu_time);\n",
    "    \n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 vector_add_timed.cu -o vector_add_timed && ./vector_add_timed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89f2d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Python Comparison (Optional)\n",
    "\n",
    "Here's the same operation in Python/Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba -q\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "@cuda.jit\n",
    "def vector_add_python(a, b, c):\n",
    "    i = cuda.grid(1)  # Same as blockIdx.x * blockDim.x + threadIdx.x\n",
    "    if i < c.size:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "# Test\n",
    "n = 1000000\n",
    "a = np.ones(n, dtype=np.float32)\n",
    "b = np.full(n, 2.0, dtype=np.float32)\n",
    "c = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "# Move to GPU\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "# Launch\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "vector_add_python[blocks, threads](d_a, d_b, d_c)  # Note: [blocks, threads] syntax\n",
    "\n",
    "# Get result\n",
    "result = d_c.copy_to_host()\n",
    "print(f\"Python result: {result[:10]}\")\n",
    "print(f\"‚úÖ All 3.0? {np.allclose(result, 3.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49614e7",
   "metadata": {},
   "source": [
    "### Key Difference: C++ vs Python\n",
    "\n",
    "| C++ | Python (Numba) |\n",
    "|-----|----------------|\n",
    "| `cudaMalloc(&d_a, size)` | `d_a = cuda.to_device(a)` |\n",
    "| `cudaMemcpy(..., HostToDevice)` | (automatic with `to_device`) |\n",
    "| `kernel<<<blocks, threads>>>()` | `kernel[blocks, threads]()` |\n",
    "| `cudaMemcpy(..., DeviceToHost)` | `d_a.copy_to_host()` |\n",
    "| `cudaFree(d_a)` | (automatic garbage collection) |\n",
    "\n",
    "C++ gives you **more control**, Python is **more convenient**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Exercise\n",
    "\n",
    "[Exercise 04: 2D Grid Indexing](../ex04-2d-indexing/colab-2d-indexing.ipynb) - Work with matrices!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
